{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 3 Regression Trees (LEAFToolbox - SL2P + ALR)\n",
    "\n",
    "This notebook contains code blocks to generate predictions based on three different treatment methods, which are as follows:\n",
    "1. SL2P10 – using only the output from SL2P10_10m\n",
    "2. LARS + Regression Tree – using feature selection (LARS) and smileCART (GEE function)\n",
    "3. LARS + Neural Network – as implemented by Hemit in ALR_client_side\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=HTqmj1EgxF7gJVgEyh075qp7WYuqYgNri3e25eGcXro&tc=ofuGySug9yHZV8RO0tjWND5TOddouHlcV3fGW_1WLqs&cc=C7niKa71xYmUYwFAHHinpi7P2WpZjBJNzX7Jo2FWuVA>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=HTqmj1EgxF7gJVgEyh075qp7WYuqYgNri3e25eGcXro&tc=ofuGySug9yHZV8RO0tjWND5TOddouHlcV3fGW_1WLqs&cc=C7niKa71xYmUYwFAHHinpi7P2WpZjBJNzX7Jo2FWuVA</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AWgavdc8D6n2G__SJX1Gex2XS1J7jzkfgIK2fH8e_1AZ3zypmPOFm3IK42c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ee\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium  \n",
    "from folium import plugins\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy ; from scipy import stats\n",
    "import scipy.io as sio\n",
    "import sklearn as skl ; from sklearn import linear_model ; from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "\n",
    "# import custom modules (files must be in same directory as this notebook)\n",
    "import feature_collections as fc\n",
    "import image_bands as ib\n",
    "import wrapper_nets as wn\n",
    "import ee_functions as ee_func\n",
    "import ALR_functions as alr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.__version__)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### for accessing earth engine asset\n",
    "# ee.Initialize()\n",
    "# ee.Authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Prelim: Define dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# SELECT INPUT PARAMETERS\n",
    "# -----------------------\n",
    "\n",
    "# variable name\n",
    "# one of: 'fAPAR', 'fCOVER', 'LAI'\n",
    "outputName = 'LAI'\n",
    "# outputName = 'fAPAR'\n",
    "# outputName = 'fCOVER'\n",
    "\n",
    "# site selection\n",
    "# one of: 'Geraldton', 'FoxCreek', 'Kouchibouguac', 'Ottawa',\n",
    "#         'Wabush', 'QueenCharlotte', 'Attawapiskat', 'Eastmain', 'Charlottetown', 'RedBay', 'EaglePlain', 'Kitchener'\n",
    "# siteSelect = 'Eastmain'\n",
    "# siteSelect = 'RedBay'\n",
    "# siteSelect = 'QueenCharlotte'\n",
    "siteSelect = 'FoxCreek'\n",
    "# siteSelect = 'Ottawa'\n",
    "# siteSelect = 'Kouchibouguac'\n",
    "# siteSelect = 'Kitchener'\n",
    "# siteSelect = 'Wabush'\n",
    "# siteSelect = 'Geraldton'\n",
    "# siteSelect = 'Attawapiskat'\n",
    "# siteSelect = 'Charlottetown'\n",
    "# assetfolder='users/ganghong/ALR'\n",
    "# cloud_folder = 'projects/ccmeo-ag-000008/assets/ALR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# set parameters based on user-defined parameters above\n",
    "# -----------------------------------------------------\n",
    "outputParams = {\n",
    "    'fAPAR': {\n",
    "        'outputScale': 1000,\n",
    "        'outputOffset': 0,\n",
    "        'outputMax': 1\n",
    "    },\n",
    "    'fCOVER': {\n",
    "        'outputScale': 1000,\n",
    "        'outputOffset': 0,\n",
    "        'outputMax': 1\n",
    "    },\n",
    "    'LAI': {\n",
    "        'outputScale': 1000,\n",
    "        'outputOffset': 0,\n",
    "        'outputMax': 8\n",
    "    }\n",
    "}\n",
    "\n",
    "outputScale = outputParams[outputName]['outputScale']\n",
    "outputOffset = outputParams[outputName]['outputOffset']\n",
    "outputMax = outputParams[outputName]['outputMax']\n",
    "responseBand = 'estimate'+outputName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "siteParams = {\n",
    "    # Geraldton, ON\n",
    "    'Geraldton': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200811T164849_20200811T165525_T16UEA'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-86, 49.5], \\\n",
    "                          [-86, 50], \\\n",
    "                          [-85.5, 50], \\\n",
    "                          [-85.5, 49.5]]]),\n",
    "        'mapCenter': [-85.75, 49.75]\n",
    "    },\n",
    "    # Fox Creek, AB\n",
    "    'FoxCreek': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20210825T185919_20210825T190431_T11UNA'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-117, 54], \\\n",
    "                          [-117, 55], \\\n",
    "                          [-115, 55], \\\n",
    "                          [-115, 54]]]),\n",
    "        'mapCenter': [-116.8, 54.4]\n",
    "    },\n",
    "    # Kouchibouguac, NB\n",
    "    'Kouchibouguac': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200905T151701_20200905T151829_T20TLS'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-65, 46], \\\n",
    "                          [-65, 47], \\\n",
    "                          [-64, 47], \\\n",
    "                          [-64, 46]]]),\n",
    "        'mapCenter': [-64.5, 46.5]\n",
    "    },\n",
    "    # Ottawa, ON\n",
    "    'Ottawa': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200801T155911_20200801T160644_T18TVQ'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-75, 45], \\\n",
    "                          [-75, 46], \\\n",
    "                          [-74, 46], \\\n",
    "                          [-74, 45]]]),\n",
    "        'mapCenter': [-74.5, 45.5]\n",
    "    },\n",
    "    # Wabush, NL\n",
    "    'Wabush': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200815T153911_20200815T154107_T19UFU'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-67.5, 52.3], \\\n",
    "                          [-67.5, 53.2], \\\n",
    "                          [-66.3, 53.2], \\\n",
    "                          [-66.3, 52.3]]]),\n",
    "        'mapCenter': [-67, 52.8]\n",
    "    },\n",
    "    # Queen Charlotte Island, BC\n",
    "    'QueenCharlotte': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200909T194951_20200909T195633_T08UPE'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-133, 53.2], \\\n",
    "                          [-133, 54], \\\n",
    "                          [-132, 54], \\\n",
    "                          [-132, 53.2]]]),\n",
    "        'mapCenter': [-132.4, 53.6]\n",
    "    },\n",
    "    # Attawapiskat, ON\n",
    "    'Attawapiskat': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200815T162839_20200815T163731_T17ULU'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-83, 52.3], \\\n",
    "                          [-83, 53.2], \\\n",
    "                          [-82.4, 53.2], \\\n",
    "                          [-82.4, 52.3]]]),\n",
    "        'mapCenter': [-82.7, 52.7]\n",
    "    },\n",
    "    # Eastmain, QC\n",
    "    'Eastmain': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200723T161829_20200723T162656_T17UPT'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-79.5, 51.4], \\\n",
    "                          [-79.5, 52.3], \\\n",
    "                          [-78, 52.3], \\\n",
    "                          [-78, 51.4]]]),\n",
    "        'mapCenter': [-78.7, 51.8]\n",
    "    },\n",
    "    # Charlottetown, PEI\n",
    "    'Charlottetown': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200622T151659_20200622T151653_T20TMS'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-63.3, 46.1], \\\n",
    "                          [-63.3, 46.5], \\\n",
    "                          [-62.9, 46.5], \\\n",
    "                          [-62.9, 46.1]]]),\n",
    "        'mapCenter': [-63.1, 46.3]\n",
    "    },\n",
    "    # Red Bay, NL\n",
    "    'RedBay': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200716T145729_20200716T145730_T21UWT'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-56.6, 51.6], \\\n",
    "                          [-56.6, 52.3], \\\n",
    "                          [-55.6, 52.3], \\\n",
    "                          [-56.6, 51.6]]]),\n",
    "        'mapCenter': [-56, 52]\n",
    "    },\n",
    "    # Eagle Plain, YT\n",
    "    'EaglePlain': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200731T204019_20200731T204021_T08WMU'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-137, 65.75], \\\n",
    "                          [-137, 66.5], \\\n",
    "                          [-135, 66.5], \\\n",
    "                          [-135, 65.75]]]),\n",
    "        'mapCenter': [-136.3, 66.5]\n",
    "    },\n",
    "    # Kitchener, ON\n",
    "    'Kitchener': {\n",
    "        'testImage': ee.Image('COPERNICUS/S2_SR/20200615T160911_20200615T161838_T17TNJ'),\n",
    "        'mapBounds': ee.Geometry.Polygon( \\\n",
    "                        [[[-81, 43.3], \\\n",
    "                          [-81, 44], \\\n",
    "                          [-79.8, 44], \\\n",
    "                          [-79.8, 43.3]]]),\n",
    "        'mapCenter': [-80.5, 43.7]\n",
    "    }\n",
    "}\n",
    "\n",
    "mapBounds = siteParams[siteSelect]['mapBounds']\n",
    "mapCenter = siteParams[siteSelect]['mapCenter']\n",
    "testImage = siteParams[siteSelect]['testImage']\n",
    "\n",
    "# other filters\n",
    "maxCloudcover = 10\n",
    "\n",
    "# export parameters\n",
    "exportFolder = siteSelect+'_'+outputName\n",
    "exportDataType = 'int'\n",
    "exportScale = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COLLECTION_OPTIONS = {\n",
    "    # Sentinel 2 using 20 m bands:\n",
    "    'COPERNICUS/S2_SR': {\n",
    "      \"name\": 'COPERNICUS/S2_SR',\n",
    "      \"description\": 'Sentinel 2A',\n",
    "      \"Cloudcover\": 'CLOUDY_PIXEL_PERCENTAGE',\n",
    "      \"Watercover\": 'WATER_PERCENTAGE',\n",
    "      \"sza\": 'MEAN_SOLAR_ZENITH_ANGLE',\n",
    "      \"vza\": 'MEAN_INCIDENCE_ZENITH_ANGLE_B8A',\n",
    "      \"saa\": 'MEAN_SOLAR_AZIMUTH_ANGLE', \n",
    "      \"vaa\": 'MEAN_INCIDENCE_AZIMUTH_ANGLE_B8A',\n",
    "      \"VIS_OPTIONS\": 'VIS_OPTIONS',\n",
    "      \"Collection_SL2P\": ee.FeatureCollection(fc.s2_createFeatureCollection_estimates()),\n",
    "      \"Collection_SL2Perrors\": ee.FeatureCollection(fc.s2_createFeatureCollection_errors()),  \n",
    "      \"sl2pDomain\": ee.FeatureCollection(fc.s2_createFeatureCollection_domains()),\n",
    "      \"Network_Ind\": ee.FeatureCollection(fc.s2_createFeatureCollection_Network_Ind()),\n",
    "      \"partition\": ee.ImageCollection(fc.s2_createImageCollection_partition()),\n",
    "      \"legend\": ee.FeatureCollection(fc.s2_createFeatureCollection_legend()),\n",
    "      \"numVariables\": 7\n",
    "    },\n",
    "    # Sentinel 2 using 10 m bands:\n",
    "    'COPERNICUS/S2_SR_10m': {\n",
    "      \"name\": 'COPERNICUS/S2_SR',\n",
    "      \"description\": 'Sentinel 2A',\n",
    "      \"Cloudcover\": 'CLOUDY_PIXEL_PERCENTAGE',\n",
    "      \"Watercover\": 'WATER_PERCENTAGE',\n",
    "      \"sza\": 'MEAN_SOLAR_ZENITH_ANGLE',\n",
    "      \"vza\": 'MEAN_INCIDENCE_ZENITH_ANGLE_B8A',\n",
    "      \"saa\": 'MEAN_SOLAR_AZIMUTH_ANGLE', \n",
    "      \"vaa\": 'MEAN_INCIDENCE_AZIMUTH_ANGLE_B8A',\n",
    "      \"VIS_OPTIONS\": 'VIS_OPTIONS',\n",
    "      \"Collection_SL2P\": ee.FeatureCollection(fc.s2_10m_createFeatureCollection_estimates()),\n",
    "      \"Collection_SL2Perrors\": ee.FeatureCollection(fc.s2_10m_createFeatureCollection_errors()),  \n",
    "      \"sl2pDomain\": ee.FeatureCollection(fc.s2_10m_createFeatureCollection_domains()),\n",
    "      \"Network_Ind\": ee.FeatureCollection(fc.s2_createFeatureCollection_Network_Ind()),\n",
    "      \"partition\": ee.ImageCollection(fc.s2_createImageCollection_partition()),\n",
    "      \"legend\": ee.FeatureCollection(fc.s2_createFeatureCollection_legend()),\n",
    "      \"numVariables\": 7\n",
    "    }\n",
    "}\n",
    "\n",
    "VIS_OPTIONS = {\n",
    "    'fAPAR': {\n",
    "        \"COPERNICUS/S2_SR\": {\n",
    "            \"Name\": 'fAPAR',\n",
    "            \"errorName\": 'errorfAPAR',\n",
    "            \"maskName\": 'maskfAPAR',\n",
    "            \"description\": 'Fraction of absorbed photosynthetically active radiation',\n",
    "            \"variable\": 2,\n",
    "            \"inputBands\":      ['cosVZA', 'cosSZA', 'cosRAA', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12'],\n",
    "            \"inputScaling\":    [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            \"outmin\": (ee.Image(ee.Array([[0]]))),\n",
    "            \"outmax\": (ee.Image(ee.Array([[1]])))\n",
    "        },\n",
    "        \"COPERNICUS/S2_SR_10m\": {\n",
    "            \"Name\": 'fAPAR',\n",
    "            \"errorName\": 'errorfAPAR',\n",
    "            \"maskName\": 'maskfAPAR',\n",
    "            \"description\": 'Fraction of absorbed photosynthetically active radiation',\n",
    "            \"variable\": 2,\n",
    "            \"inputBands\":      ['cosVZA', 'cosSZA', 'cosRAA', 'B2', 'B3', 'B4', 'B8'],\n",
    "            \"inputScaling\":    [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            \"outmin\": (ee.Image(ee.Array([[0]]))),\n",
    "            \"outmax\": (ee.Image(ee.Array([[1]])))\n",
    "        }\n",
    "    },\n",
    "    'fCOVER': {\n",
    "        \"COPERNICUS/S2_SR\": {\n",
    "            \"Name\": 'fCOVER',\n",
    "            \"errorName\": 'errorfCOVER',\n",
    "            \"maskName\": 'maskfCOVER',\n",
    "            \"description\": 'Fraction of canopy cover',\n",
    "            \"variable\": 3,\n",
    "            \"inputBands\":      ['cosVZA', 'cosSZA', 'cosRAA', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12'],\n",
    "            \"inputScaling\":    [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            \"outmin\": (ee.Image(ee.Array([[0]]))),\n",
    "            \"outmax\": (ee.Image(ee.Array([[1]]))) \n",
    "        },\n",
    "        \"COPERNICUS/S2_SR_10m\": {\n",
    "            \"Name\": 'fCOVER',\n",
    "            \"errorName\": 'errorfCOVER',\n",
    "            \"maskName\": 'maskfCOVER',\n",
    "            \"description\": 'Fraction of canopy cover',\n",
    "            \"variable\": 3,\n",
    "            \"inputBands\":      ['cosVZA', 'cosSZA', 'cosRAA', 'B2', 'B3', 'B4', 'B8'],\n",
    "            \"inputScaling\":    [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            \"outmin\": (ee.Image(ee.Array([[0]]))),\n",
    "            \"outmax\": (ee.Image(ee.Array([[1]]))) \n",
    "        }\n",
    "    },\n",
    "    'LAI': {\n",
    "        \"COPERNICUS/S2_SR\": {\n",
    "            \"Name\": 'LAI',\n",
    "            \"errorName\": 'errorLAI',\n",
    "            \"maskName\": 'maskLAI',\n",
    "            \"description\": 'Leaf area index',\n",
    "            \"variable\": 1,\n",
    "            \"inputBands\":      ['cosVZA', 'cosSZA', 'cosRAA', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B11', 'B12'],\n",
    "            \"inputScaling\":    [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            \"outmin\": (ee.Image(ee.Array([[0]]))),\n",
    "            \"outmax\": (ee.Image(ee.Array([[1]])))\n",
    "        },\n",
    "        \"COPERNICUS/S2_SR_10m\": {\n",
    "            \"Name\": 'LAI',\n",
    "            \"errorName\": 'errorLAI',\n",
    "            \"maskName\": 'maskLAI',\n",
    "            \"description\": 'Leaf area index',\n",
    "            \"variable\": 1,\n",
    "            \"inputBands\":      ['cosVZA', 'cosSZA', 'cosRAA', 'B2', 'B3', 'B4', 'B8'],\n",
    "            \"inputScaling\":    [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            \"outmin\": (ee.Image(ee.Array([[0]]))),\n",
    "            \"outmax\": (ee.Image(ee.Array([[1]])))\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## ALR function for estimation\n",
    "# def func_ALR(tmp_image, responseBand, outputName, mapBounds):\n",
    "#     #inputImage = ee.Image(cloud_folder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "#     inputImage = ee.Image(tmp_image).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "#     # responseBand=response_Band\n",
    "#     # outputName=output_Name\n",
    "#     # mapBounds= map_Bounds\n",
    "    \n",
    "#     inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'QA60', 'date', 'estimate'+outputName, 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "#     inputImage = inputImage.rename(inputImage_bands)\n",
    "#     input_VI_definition = ee.List([\n",
    "#                                    \"GI      = b('B3')/b('B4')\",                               \n",
    "#                                    \"SGI     = b('B8')/b('B4')\",                                \n",
    "#                                    \"GVI     = (b('B8')/b('B3'))-1\",                             \n",
    "#                                    \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",                                \n",
    "#                                    \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "#                                    \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",                                \n",
    "#                                    \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",                                 \n",
    "#                                    \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "#                                    \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "#                                    \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "#                                    \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",                            \n",
    "#                                    \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",                                \n",
    "#                                    \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])\n",
    "\n",
    "#     # names of bands to pass to ALR method (excluding metadata and other non-spectral bands)\n",
    "#     input_bandNames = ['B2', 'B3', 'B4', 'B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI']\n",
    "    \n",
    "#     def format_image(image, image_bands, response_band, VI_definition):\n",
    "#         image = ee.Image(image)\n",
    "#         image_bands = ee.List(image_bands)\n",
    "#         response_band = ee.String(response_band)\n",
    "#         VI_definition = ee.List(VI_definition)\n",
    "        \n",
    "#         # image_bands specifices a list of the names of the bands used in defining the expressions for VIs in VI_definition\n",
    "#         image = image.rename(image_bands).toDouble()\n",
    "        \n",
    "#         # Generate an ImageCollection from a list of expressions defining a set of VIs using the bands available in the image\n",
    "#         VIimageCollection = ee.ImageCollection(VI_definition.map(lambda expr: image.expression(expr)))\n",
    "#         VIimage = VIimageCollection.toBands().regexpRename(\"[0-9]+_\", \"\")\n",
    "        \n",
    "#         # Reorder the bands in the image so the response band is the first band in the image\n",
    "#         feature_bands = image_bands.remove(response_band)\n",
    "        \n",
    "#         return ee.Image(image.select(response_band).addBands(VIimage).addBands(image.select(feature_bands)))\n",
    "\n",
    "    \n",
    "#     inputImage = format_image(inputImage, inputImage_bands, responseBand, input_VI_definition)\n",
    "    \n",
    "#     def scale_image(image, response_band):\n",
    "#         image = ee.Image(image)\n",
    "#         response_band = ee.String(response_band)\n",
    "        \n",
    "#         # def get_num_pixels(image):\n",
    "            \n",
    "#         image_dimensions = ee.List(image.getInfo()['bands'][0]['dimensions'])\n",
    "#         image_height = image_dimensions.getNumber(0)\n",
    "#         image_width = image_dimensions.getNumber(1)\n",
    "#         image_pixels = image_height.multiply(image_width)\n",
    "    \n",
    "# #             # get image height\n",
    "# #             def get_height(image):\n",
    "# #                 # height = image.getInfo()[\"bands\"][0][\"dimensions\"][0]\n",
    "# #                 height = image.get('bands')[0][\"dimensions\"][0]\n",
    "# #                 return height\n",
    "\n",
    "# #             # get image width\n",
    "# #             def get_width(image):\n",
    "# #                 # width = image.getInfo()[\"bands\"][0][\"dimensions\"][1]\n",
    "# #                 width = image.get('bands')[0][\"dimensions\"][1]\n",
    "# #                 return width\n",
    "\n",
    "#             # image_height = get_height(image)\n",
    "#             # image_width = get_width(image)\n",
    "#             # image_pixels = image_height*image_width\n",
    "\n",
    "# #             return image_pixels\n",
    "        \n",
    "# #         image_pixels = ee.Number(get_num_pixels(image))\n",
    "        \n",
    "#         # Set up lists containing the input/feature bands in the image\n",
    "#         bandList = image.bandNames()\n",
    "#         featureList = bandList.remove(response_band)\n",
    "#         num_bands = bandList.length()\n",
    "#         num_features = featureList.length()\n",
    "        \n",
    "#         # We will be using the reduceRegion() function on images from Earth Engine, \n",
    "#         # which will process up to a specified number of pixels from the image to generate the outputs of the reducer\n",
    "#         max_pixels = image_pixels.min(10000000)\n",
    "#         # best_effort = ee.Algorithms.If(image_pixels.gt(max_pixels), True, False)\n",
    "        \n",
    "#         # Set default projection and scale using the response band\n",
    "#         defaultScale = image.select(response_band).projection().nominalScale()\n",
    "#         defaultCrs = image.select(response_band).projection().crs()\n",
    "#         image = image.setDefaultProjection(crs=defaultCrs, scale=defaultScale)\n",
    "        \n",
    "#         # Center all of the bands in the image for LARs\n",
    "#         # We will centre the sampled data later as well as reduceRegion() is not precise enough\n",
    "#         meanImage = image.subtract(image.reduceRegion(reducer=ee.Reducer.mean(), \\\n",
    "#                                     scale=defaultScale, bestEffort=True, maxPixels=max_pixels).toImage(bandList))\n",
    "        \n",
    "#         # Separate the image into features (X) and response (y) as we need to standardize the input features\n",
    "#         X = meanImage.select(featureList)\n",
    "#         y = meanImage.select(response_band)\n",
    "        \n",
    "#         # Standardize the input features\n",
    "#         X = X.divide(X.reduceRegion(reducer=ee.Reducer.stdDev(), bestEffort=True, maxPixels=max_pixels).toImage(featureList))\n",
    "        \n",
    "#         return X.addBands(y)\n",
    "\n",
    "    \n",
    "#     scaledImage = scale_image(inputImage, responseBand)\n",
    "    \n",
    "    \n",
    "#     def ee_LARS(input_image, input_bandNames, response_bandName, num_nonzero_coefficients, num_samples):\n",
    "#         image = ee.Image(input_image)\n",
    "#         feature_list = ee.List(input_bandNames)\n",
    "#         response_band = ee.String(response_bandName)\n",
    "#         full_band_list = ee.List(feature_list).add(response_band)\n",
    "#         num_nonzero_coefficients = ee.Number(num_nonzero_coefficients)\n",
    "#         num_samples = ee.Number(num_samples)\n",
    "#         # def get_num_pixels(image):\n",
    "            \n",
    "#         image_dimensions = ee.List(image.getInfo()['bands'][0]['dimensions'])\n",
    "#         image_height = image_dimensions.getNumber(0)\n",
    "#         image_width = image_dimensions.getNumber(1)\n",
    "#         image_pixels = image_height.multiply(image_width)\n",
    "    \n",
    "# #             # get image height\n",
    "# #             def get_height(image):\n",
    "# #                 # height = image.getInfo()[\"bands\"][0][\"dimensions\"][0]\n",
    "# #                 height = image.get('bands')[0][\"dimensions\"][0]\n",
    "# #                 return height\n",
    "\n",
    "# #             # get image width\n",
    "# #             def get_width(image):\n",
    "# #                 # width = image.getInfo()[\"bands\"][0][\"dimensions\"][1]\n",
    "# #                 width = image.get('bands')[0][\"dimensions\"][1]\n",
    "# #                 return width\n",
    "\n",
    "# #             image_height = get_height(image)\n",
    "# #             image_width = get_width(image)\n",
    "# #             image_pixels = image_height*image_width\n",
    "\n",
    "#             # return image_pixels\n",
    "#         # image_pixels = ee.Number(get_num_pixels(image))\n",
    "        \n",
    "#         # Randomly sample pixels in the image at native resolution into a FeatureCollection\n",
    "#         input_collection = image.sample(numPixels=num_samples.min(image_pixels))\n",
    "#         n = input_collection.size()\n",
    "#         m = feature_list.length()\n",
    "        \n",
    "#         # Use an aggregate array function over the FeatureCollection and map the function over each feature in the band list\n",
    "#         # to generate a dictionary of all of the samples retrieved\n",
    "#         inputs = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_array(feature)))\n",
    "        \n",
    "#         # Although we may call our scale_image function on the input image, the reduceRegion() function used to determine the mean\n",
    "#         # and standard deviation of each band in the image over the entire region is not precise enough over a large image\n",
    "#         # so we must recenter all of the bands in the image and now we can also normalize (L2 norm) each input feature as required\n",
    "#         # by the LARs algorithm\n",
    "        \n",
    "#         # Use an aggregate_mean function over the feature collection to get the mean of each band\n",
    "#         input_means = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_mean(feature)))\n",
    "\n",
    "#         def centre_inputs(key, value):\n",
    "#             key_mean = input_means.getNumber(key)\n",
    "#             return ee.List(value).map(lambda sample: ee.Number(sample).subtract(key_mean))\n",
    "        \n",
    "        \n",
    "#         # Center bands by mapping over the list of features and then a subtracting over the list of samples for each band\n",
    "#         inputs = inputs.map(centre_inputs)\n",
    "\n",
    "#         # Separate the response variable samples into its own vector\n",
    "#         y = inputs.toArray([response_band]).reshape([-1,1])\n",
    "\n",
    "#         # Remove response band from the feature collection by selecting only bands in the feature list\n",
    "#         inputs = inputs.select(feature_list)\n",
    "        \n",
    "#         # Generate a dictionary of all of the L2 norms of the input features using a custom mapped function\n",
    "#         input_norms = inputs.map(lambda key, value: ee.Number(ee.List(value).map(lambda sample: ee.Number(sample).pow(2)).reduce(ee.Reducer.sum())).pow(0.5))\n",
    "\n",
    "#         def norm_inputs(key, value):\n",
    "#             key_norm = input_norms.getNumber(key)\n",
    "#             return ee.List(value).map(lambda sample: ee.Number(sample).divide(key_norm))\n",
    "        \n",
    "#         # Normalize all of the features by mapping a function over the list of features\n",
    "#         # and then map a division over the list of all of the samples of the feature\n",
    "#         inputs = inputs.map(norm_inputs)\n",
    "        \n",
    "#         # Generate the array of samples using the dictionary\n",
    "#         X = inputs.toArray(feature_list).transpose()\n",
    "\n",
    "#         # Find the first best predictor of the response to initialize the main LARs loop\n",
    "#         initial_prediction = ee.Array(ee.List.repeat([0], n))\n",
    "#         c = X.transpose().matrixMultiply(y.subtract(initial_prediction))\n",
    "#         c_abs = c.abs()\n",
    "#         C_maxLoc = c_abs.project([0]).argmax()\n",
    "#         add_feature = C_maxLoc.getNumber(0)\n",
    "#         A = ee.List([add_feature])\n",
    "        \n",
    "#         # Create a dicitionary of initial inputs to pass into the main LARs iterative loop\n",
    "#         # The iterate function in Earth Engine processes each iteration as a tree of iterations with no access to any variables\n",
    "#         # from previous iterations (only those that are passed to the next iteration)\n",
    "#         # so we must pass both the current prediction and the active set of features (with non-zero coefficients), A\n",
    "#         initial_inputs = ee.Dictionary({'prediction': initial_prediction, 'A': A})\n",
    "\n",
    "#         def LARs_regression(iteration, inputs):\n",
    "#             inputs = ee.Dictionary(inputs)\n",
    "\n",
    "#             # Find the active set of features, A (predictors with non-zero coefficients)\n",
    "#             A = ee.List(inputs.get('A'))\n",
    "#             # A_list is an array used to mask the full array of input samples and the correlation vector\n",
    "#             A_list = ee.Array(ee.List.sequence(0, m.subtract(1))\\\n",
    "#                               .map(lambda index: A.contains(index)).replaceAll(False, 0).replaceAll(True, 1)).reshape([-1,1])\n",
    "\n",
    "#             # The following matrix algebra determines the next most correlated variable, or the next best predictor considering the\n",
    "#             # current features in the active set, A, as well as the magnitude to adjust the prediction vector to ensure all of the\n",
    "#             # features in the active set are equally correlated to response vector\n",
    "#             prediction = inputs.getArray('prediction')\n",
    "#             c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "#             c_abs = c.abs()\n",
    "#             C_max = c_abs.get(c_abs.argmax())\n",
    "#             s_A = c.divide(c_abs).mask(A_list)\n",
    "#             X_A = X.mask(A_list.transpose())\n",
    "#             G_Ai = X_A.transpose().matrixMultiply(X_A).matrixInverse()\n",
    "#             G1 = G_Ai.matrixMultiply(s_A)\n",
    "#             A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "#             w_A = G1.multiply(A_A)\n",
    "#             u_A = X_A.matrixMultiply(w_A)\n",
    "#             a = X.transpose().matrixMultiply(u_A)\n",
    "#             a = a.project([0])\n",
    "#             c = c.project([0])\n",
    "\n",
    "#             def compute_gammaArray(index_j):\n",
    "#                 minus_j = C_max.subtract(c.get([index_j])).divide(A_A.subtract(a.get([index_j])))\n",
    "#                 plus_j = C_max.add(c.get([index_j])).divide(A_A.add(a.get([index_j])))\n",
    "#                 return ee.List([minus_j, plus_j]).filter(ee.Filter.gte('item', 0)).reduce(ee.Reducer.min())\n",
    "\n",
    "#             A_c = ee.List.sequence(0, m.subtract(1)).removeAll(A)\n",
    "#             gammaArray = A_c.map(compute_gammaArray)\n",
    "#             gamma = gammaArray.reduce(ee.Reducer.min())\n",
    "#             min_location = gammaArray.indexOf(gamma)\n",
    "#             add_feature = A_c.getNumber(min_location)\n",
    "\n",
    "#             # Update active set of variables with next best predictor from non-active set and update prediction vector\n",
    "#             A = A.add(add_feature)\n",
    "#             prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "#             return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "\n",
    "#         # The final iteration of LARs (if selecting all input variables) requires a different method to determine magnitude for\n",
    "#         # adjusting the magnitude of the prediction vector, as the regular LARs iteration relies on variables in non-active set\n",
    "#         # In the final iteration there will be no variables in the non-active set, so the method will not work\n",
    "#         def LARs_final_iteration(iteration, inputs):\n",
    "#             inputs = ee.Dictionary(inputs)\n",
    "#             A = ee.List(inputs.get('A'))\n",
    "\n",
    "#             prediction = inputs.getArray('prediction')\n",
    "#             c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "#             c_abs = c.abs()\n",
    "#             C_max = c_abs.get(c_abs.argmax())        \n",
    "\n",
    "#             s_A = c.divide(c_abs)\n",
    "#             G_Ai = X.transpose().matrixMultiply(X).matrixInverse()\n",
    "#             G1 = G_Ai.matrixMultiply(s_A)\n",
    "#             A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "#             w_A = G1.multiply(A_A)\n",
    "#             u_A = X.matrixMultiply(w_A)\n",
    "\n",
    "#             gamma = C_max.divide(A_A)\n",
    "#             prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "#             return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "#         # Actually carrying out the iterations by iterating over a placeholder list (sequence from 1 to the number of non-zero\n",
    "#         # variables that the user wishes to select as predictors for the response)\n",
    "#         iterations = ee.List.sequence(1, m.subtract(1).min(num_nonzero_coefficients))\n",
    "#         penultimate_outputs = iterations.iterate(LARs_regression, initial_inputs)\n",
    "#         final_outputs = ee.Dictionary(ee.Algorithms.If(num_nonzero_coefficients.gte(m), \\\n",
    "#                                 LARs_final_iteration(m, penultimate_outputs), penultimate_outputs))\n",
    "        \n",
    "#         final_prediction = final_outputs.getArray('prediction')\n",
    "\n",
    "#         A = ee.List(final_outputs.get('A'))\n",
    "\n",
    "#         feature_path = A.slice(0, num_nonzero_coefficients).map(lambda index: feature_list.getString(index))\n",
    "\n",
    "#         # The code snippet below is able to extract the exact coefficients on all of the selected features, but is commented out\n",
    "#         # as it adds computational complexity that takes up unnecessary memory on the Google Earth Engine virtual machine since we\n",
    "#         # are only using LARs as a feature selection algorithm\n",
    "\n",
    "#     #     coefficients = X.matrixSolve(final_prediction).project([0])\\\n",
    "#     #                               .toList().map(lambda num: ee.Algorithms.If(ee.Number(num).abs().lt(0.001), 0, num))\n",
    "#     #     print('Coefficients')\n",
    "#     #     coeff = ee.Dictionary.fromLists(featureList, coefficients).getInfo()\n",
    "#     #     ordered_coeff = OrderedDict()\n",
    "#     #     var_path = feature_path.cat(featureList.removeAll(feature_path)).getInfo()\n",
    "#     #     for key in var_path:\n",
    "#     #         ordered_coeff[key] = coeff[key]\n",
    "#     #     print(json.dumps(ordered_coeff, indent=1))\n",
    "        \n",
    "#         # print('selected features: ', feature_path.getInfo())\n",
    "        \n",
    "#         return feature_path\n",
    "\n",
    "    \n",
    "#     select_features = ee_LARS(scaledImage, input_bandNames, responseBand, 5, 50000)\n",
    "#     #unclassified = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_VI')\n",
    "#     unclassified = ee.Image(inputImage)\n",
    "#     bands = ee.List([responseBand, 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI',\n",
    "#                      'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI', 'B2', 'B3', 'B4', 'B8',\n",
    "#                      'QA60', 'date', 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "#     unclassified = unclassified.rename(bands)\n",
    "\n",
    "#     # prediction bands (equivalent to select_features, with responseBand)\n",
    "#     bands = select_features\n",
    "#     input_bands = select_features.add(responseBand)\n",
    "#     training_data = ee.FeatureCollection(unclassified.sample(numPixels=1000, seed=1).select(input_bands))\n",
    "#     # implement regression tree with Random Forest algorithm\n",
    "#     # optional parameters for smileRandomForest(): variablesPerSplit, minLeafPopulation, bagFraction, maxNodes, seed\n",
    "#     rf_classifier = ee.Classifier.smileRandomForest(100).setOutputMode('REGRESSION').train(features=training_data,\n",
    "#                                                                                            classProperty=responseBand,\n",
    "#                                                                                            inputProperties=input_bands)\n",
    "#     rf_classified = unclassified.select(bands).classify(rf_classifier, 'ALR_'+responseBand).clip(mapBounds)\n",
    "#     return tmp_image.addBands(rf_classified)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ALR for server side processing through map function\n",
    "# def func1_ALR(responseBand, outputName, mapBounds):\n",
    "#     def wrap(img):\n",
    "#         #inputImage = ee.Image(cloud_folder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "#         inputImage = ee.Image(img).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "#         inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'QA60', 'date', 'estimate'+outputName, 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "#         inputImage = inputImage.rename(inputImage_bands)\n",
    "#         input_VI_definition = ee.List([\n",
    "#                                        \"GI      = b('B3')/b('B4')\",                               \n",
    "#                                        \"SGI     = b('B8')/b('B4')\",                                \n",
    "#                                        \"GVI     = (b('B8')/b('B3'))-1\",                             \n",
    "#                                        \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",                                \n",
    "#                                        \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "#                                        \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",                                \n",
    "#                                        \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",                                 \n",
    "#                                        \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "#                                        \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "#                                        \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "#                                        \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",                            \n",
    "#                                        \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",                                \n",
    "#                                        \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])\n",
    "\n",
    "#         # names of bands to pass to ALR method (excluding metadata and other non-spectral bands)\n",
    "#         input_bandNames = ['B2', 'B3', 'B4', 'B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI']\n",
    "\n",
    "#         def format_image(image, image_bands, response_band, VI_definition):\n",
    "#             image = ee.Image(image)\n",
    "#             image_bands = ee.List(image_bands)\n",
    "#             response_band = ee.String(response_band)\n",
    "#             VI_definition = ee.List(VI_definition)\n",
    "\n",
    "#             # image_bands specifices a list of the names of the bands used in defining the expressions for VIs in VI_definition\n",
    "#             # image = image.rename(image_bands).toDouble()\n",
    "#             image = image.toDouble()\n",
    "\n",
    "#             # Generate an ImageCollection from a list of expressions defining a set of VIs using the bands available in the image\n",
    "#             VIimageCollection = ee.ImageCollection(VI_definition.map(lambda expr: image.expression(expr)))\n",
    "#             VIimage = VIimageCollection.toBands().regexpRename(\"[0-9]+_\", \"\")\n",
    "\n",
    "#             # Reorder the bands in the image so the response band is the first band in the image\n",
    "#             feature_bands = image_bands.remove(response_band)\n",
    "\n",
    "#             return ee.Image(image.select(response_band).addBands(VIimage).addBands(image.select(feature_bands)))\n",
    "\n",
    "\n",
    "#         inputImage = format_image(inputImage, inputImage_bands, responseBand, input_VI_definition)\n",
    "\n",
    "#         def scale_image(image, response_band):\n",
    "#             image = ee.Image(image)\n",
    "#             response_band = ee.String(response_band)\n",
    "\n",
    "#             # def get_num_pixels(image):\n",
    "\n",
    "#             # image_dimensions = ee.List(image.getInfo()['bands'][0]['dimensions'])\n",
    "#             # # image_dimensions = ee.List(image.get('bands')[0]['dimensions'])\n",
    "#             # image_height = image_dimensions.getNumber(0)\n",
    "#             # image_width = image_dimensions.getNumber(1)\n",
    "#             # image_pixels = image_height.multiply(image_width)  \n",
    "            \n",
    "#             imgDesc = ee.Algorithms.Describe(image)\n",
    "#             image_height = ee.List(ee.Dictionary(ee.List(ee.Dictionary(imgDesc).get(\"bands\") ).get(0) ).get(\"dimensions\") ).get(0)\n",
    "#             image_width = ee.List(ee.Dictionary(ee.List(ee.Dictionary(imgDesc).get(\"bands\") ).get(0) ).get(\"dimensions\") ).get(1)               \n",
    "            \n",
    "#             image_pixels = ee.Number(image_height).multiply(ee.Number(image_width))\n",
    "\n",
    "#             # Set up lists containing the input/feature bands in the image\n",
    "#             bandList = image.bandNames()\n",
    "#             featureList = bandList.remove(response_band)\n",
    "#             num_bands = bandList.length()\n",
    "#             num_features = featureList.length()\n",
    "\n",
    "#             # We will be using the reduceRegion() function on images from Earth Engine, \n",
    "#             # which will process up to a specified number of pixels from the image to generate the outputs of the reducer\n",
    "#             max_pixels = image_pixels.min(10000000)\n",
    "#             # best_effort = ee.Algorithms.If(image_pixels.gt(max_pixels), True, False)\n",
    "\n",
    "#             # Set default projection and scale using the response band\n",
    "#             defaultScale = image.select(response_band).projection().nominalScale()\n",
    "#             defaultCrs = image.select(response_band).projection().crs()\n",
    "#             image = image.setDefaultProjection(crs=defaultCrs, scale=defaultScale)\n",
    "\n",
    "#             # Center all of the bands in the image for LARs\n",
    "#             # We will centre the sampled data later as well as reduceRegion() is not precise enough\n",
    "#             meanImage = image.subtract(image.reduceRegion(reducer=ee.Reducer.mean(), \\\n",
    "#                                         scale=defaultScale, bestEffort=True, maxPixels=max_pixels).toImage(bandList))\n",
    "\n",
    "#             # Separate the image into features (X) and response (y) as we need to standardize the input features\n",
    "#             X = meanImage.select(featureList)\n",
    "#             y = meanImage.select(response_band)\n",
    "\n",
    "#             # Standardize the input features\n",
    "#             X = X.divide(X.reduceRegion(reducer=ee.Reducer.stdDev(), bestEffort=True, maxPixels=max_pixels).toImage(featureList))\n",
    "\n",
    "#             return X.addBands(y)\n",
    "\n",
    "\n",
    "#         scaledImage = scale_image(inputImage, responseBand)\n",
    "\n",
    "\n",
    "#         def ee_LARS(input_image, input_bandNames, response_bandName, num_nonzero_coefficients, num_samples):\n",
    "#             image = ee.Image(input_image)\n",
    "#             feature_list = ee.List(input_bandNames)\n",
    "#             response_band = ee.String(response_bandName)\n",
    "#             full_band_list = ee.List(feature_list).add(response_band)\n",
    "#             num_nonzero_coefficients = ee.Number(num_nonzero_coefficients)\n",
    "#             num_samples = ee.Number(num_samples)\n",
    "#             # def get_num_pixels(image):\n",
    "\n",
    "#             # image_dimensions = ee.List(image.getInfo()['bands'][0]['dimensions'])\n",
    "#             # image_height = image_dimensions.getNumber(0)\n",
    "#             # image_width = image_dimensions.getNumber(1)\n",
    "#             # image_pixels = image_height.multiply(image_width)\n",
    "#             imgDesc = ee.Algorithms.Describe(image)\n",
    "#             image_height = ee.List(ee.Dictionary(ee.List(ee.Dictionary(imgDesc).get(\"bands\") ).get(0) ).get(\"dimensions\") ).get(0)\n",
    "#             image_width = ee.List(ee.Dictionary(ee.List(ee.Dictionary(imgDesc).get(\"bands\") ).get(0) ).get(\"dimensions\") ).get(1)               \n",
    "            \n",
    "#             image_pixels = ee.Number(image_height).multiply(ee.Number(image_width))\n",
    "\n",
    "#             # Randomly sample pixels in the image at native resolution into a FeatureCollection\n",
    "#             input_collection = image.sample(numPixels=num_samples.min(image_pixels))\n",
    "#             n = input_collection.size()\n",
    "#             m = feature_list.length()\n",
    "\n",
    "#             # Use an aggregate array function over the FeatureCollection and map the function over each feature in the band list\n",
    "#             # to generate a dictionary of all of the samples retrieved\n",
    "#             inputs = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_array(feature)))\n",
    "\n",
    "#             # Although we may call our scale_image function on the input image, the reduceRegion() function used to determine the mean\n",
    "#             # and standard deviation of each band in the image over the entire region is not precise enough over a large image\n",
    "#             # so we must recenter all of the bands in the image and now we can also normalize (L2 norm) each input feature as required\n",
    "#             # by the LARs algorithm\n",
    "\n",
    "#             # Use an aggregate_mean function over the feature collection to get the mean of each band\n",
    "#             input_means = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_mean(feature)))\n",
    "\n",
    "#             def centre_inputs(key, value):\n",
    "#                 key_mean = input_means.getNumber(key)\n",
    "#                 return ee.List(value).map(lambda sample: ee.Number(sample).subtract(key_mean))\n",
    "\n",
    "\n",
    "#             # Center bands by mapping over the list of features and then a subtracting over the list of samples for each band\n",
    "#             inputs = inputs.map(centre_inputs)\n",
    "\n",
    "#             # Separate the response variable samples into its own vector\n",
    "#             y = inputs.toArray([response_band]).reshape([-1,1])\n",
    "\n",
    "#             # Remove response band from the feature collection by selecting only bands in the feature list\n",
    "#             inputs = inputs.select(feature_list)\n",
    "\n",
    "#             # Generate a dictionary of all of the L2 norms of the input features using a custom mapped function\n",
    "#             input_norms = inputs.map(lambda key, value: ee.Number(ee.List(value).map(lambda sample: ee.Number(sample).pow(2)).reduce(ee.Reducer.sum())).pow(0.5))\n",
    "\n",
    "#             def norm_inputs(key, value):\n",
    "#                 key_norm = input_norms.getNumber(key)\n",
    "#                 return ee.List(value).map(lambda sample: ee.Number(sample).divide(key_norm))\n",
    "\n",
    "#             # Normalize all of the features by mapping a function over the list of features\n",
    "#             # and then map a division over the list of all of the samples of the feature\n",
    "#             inputs = inputs.map(norm_inputs)\n",
    "\n",
    "#             # Generate the array of samples using the dictionary\n",
    "#             X = inputs.toArray(feature_list).transpose()\n",
    "\n",
    "#             # Find the first best predictor of the response to initialize the main LARs loop\n",
    "#             initial_prediction = ee.Array(ee.List.repeat([0], n))\n",
    "#             c = X.transpose().matrixMultiply(y.subtract(initial_prediction))\n",
    "#             c_abs = c.abs()\n",
    "#             C_maxLoc = c_abs.project([0]).argmax()\n",
    "#             add_feature = C_maxLoc.getNumber(0)\n",
    "#             A = ee.List([add_feature])\n",
    "\n",
    "#             # Create a dicitionary of initial inputs to pass into the main LARs iterative loop\n",
    "#             # The iterate function in Earth Engine processes each iteration as a tree of iterations with no access to any variables\n",
    "#             # from previous iterations (only those that are passed to the next iteration)\n",
    "#             # so we must pass both the current prediction and the active set of features (with non-zero coefficients), A\n",
    "#             initial_inputs = ee.Dictionary({'prediction': initial_prediction, 'A': A})\n",
    "\n",
    "#             def LARs_regression(iteration, inputs):\n",
    "#                 inputs = ee.Dictionary(inputs)\n",
    "\n",
    "#                 # Find the active set of features, A (predictors with non-zero coefficients)\n",
    "#                 A = ee.List(inputs.get('A'))\n",
    "#                 # A_list is an array used to mask the full array of input samples and the correlation vector\n",
    "#                 A_list = ee.Array(ee.List.sequence(0, m.subtract(1))\\\n",
    "#                                   .map(lambda index: A.contains(index)).replaceAll(False, 0).replaceAll(True, 1)).reshape([-1,1])\n",
    "\n",
    "#                 # The following matrix algebra determines the next most correlated variable, or the next best predictor considering the\n",
    "#                 # current features in the active set, A, as well as the magnitude to adjust the prediction vector to ensure all of the\n",
    "#                 # features in the active set are equally correlated to response vector\n",
    "#                 prediction = inputs.getArray('prediction')\n",
    "#                 c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "#                 c_abs = c.abs()\n",
    "#                 C_max = c_abs.get(c_abs.argmax())\n",
    "#                 s_A = c.divide(c_abs).mask(A_list)\n",
    "#                 X_A = X.mask(A_list.transpose())\n",
    "#                 G_Ai = X_A.transpose().matrixMultiply(X_A).matrixInverse()\n",
    "#                 G1 = G_Ai.matrixMultiply(s_A)\n",
    "#                 A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "#                 w_A = G1.multiply(A_A)\n",
    "#                 u_A = X_A.matrixMultiply(w_A)\n",
    "#                 a = X.transpose().matrixMultiply(u_A)\n",
    "#                 a = a.project([0])\n",
    "#                 c = c.project([0])\n",
    "\n",
    "#                 def compute_gammaArray(index_j):\n",
    "#                     minus_j = C_max.subtract(c.get([index_j])).divide(A_A.subtract(a.get([index_j])))\n",
    "#                     plus_j = C_max.add(c.get([index_j])).divide(A_A.add(a.get([index_j])))\n",
    "#                     return ee.List([minus_j, plus_j]).filter(ee.Filter.gte('item', 0)).reduce(ee.Reducer.min())\n",
    "\n",
    "#                 A_c = ee.List.sequence(0, m.subtract(1)).removeAll(A)\n",
    "#                 gammaArray = A_c.map(compute_gammaArray)\n",
    "#                 gamma = gammaArray.reduce(ee.Reducer.min())\n",
    "#                 min_location = gammaArray.indexOf(gamma)\n",
    "#                 add_feature = A_c.getNumber(min_location)\n",
    "\n",
    "#                 # Update active set of variables with next best predictor from non-active set and update prediction vector\n",
    "#                 A = A.add(add_feature)\n",
    "#                 prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "#                 return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "\n",
    "#             # The final iteration of LARs (if selecting all input variables) requires a different method to determine magnitude for\n",
    "#             # adjusting the magnitude of the prediction vector, as the regular LARs iteration relies on variables in non-active set\n",
    "#             # In the final iteration there will be no variables in the non-active set, so the method will not work\n",
    "#             def LARs_final_iteration(iteration, inputs):\n",
    "#                 inputs = ee.Dictionary(inputs)\n",
    "#                 A = ee.List(inputs.get('A'))\n",
    "\n",
    "#                 prediction = inputs.getArray('prediction')\n",
    "#                 c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "#                 c_abs = c.abs()\n",
    "#                 C_max = c_abs.get(c_abs.argmax())        \n",
    "\n",
    "#                 s_A = c.divide(c_abs)\n",
    "#                 G_Ai = X.transpose().matrixMultiply(X).matrixInverse()\n",
    "#                 G1 = G_Ai.matrixMultiply(s_A)\n",
    "#                 A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "#                 w_A = G1.multiply(A_A)\n",
    "#                 u_A = X.matrixMultiply(w_A)\n",
    "\n",
    "#                 gamma = C_max.divide(A_A)\n",
    "#                 prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "#                 return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "#             # Actually carrying out the iterations by iterating over a placeholder list (sequence from 1 to the number of non-zero\n",
    "#             # variables that the user wishes to select as predictors for the response)\n",
    "#             iterations = ee.List.sequence(1, m.subtract(1).min(num_nonzero_coefficients))\n",
    "#             penultimate_outputs = iterations.iterate(LARs_regression, initial_inputs)\n",
    "#             final_outputs = ee.Dictionary(ee.Algorithms.If(num_nonzero_coefficients.gte(m), \\\n",
    "#                                     LARs_final_iteration(m, penultimate_outputs), penultimate_outputs))\n",
    "\n",
    "#             final_prediction = final_outputs.getArray('prediction')\n",
    "\n",
    "#             A = ee.List(final_outputs.get('A'))\n",
    "\n",
    "#             feature_path = A.slice(0, num_nonzero_coefficients).map(lambda index: feature_list.getString(index))      \n",
    "\n",
    "#             return feature_path\n",
    "\n",
    "\n",
    "#         select_features = ee_LARS(scaledImage, input_bandNames, responseBand, 5, 50000)\n",
    "#         #unclassified = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_VI')\n",
    "#         unclassified = ee.Image(inputImage)\n",
    "#         bands = ee.List([responseBand, 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', \\\n",
    "#                          'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI', 'B2', 'B3', 'B4', 'B8',\\\n",
    "#                          'QA60', 'date', 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "#         unclassified = unclassified.rename(bands)\n",
    "\n",
    "#         # prediction bands (equivalent to select_features, with responseBand)\n",
    "#         bands = select_features\n",
    "#         input_bands = select_features.add(responseBand)\n",
    "#         training_data = ee.FeatureCollection(unclassified.sample(numPixels=1000, seed=1).select(input_bands))\n",
    "#         # implement regression tree with Random Forest algorithm\n",
    "#         # optional parameters for smileRandomForest(): variablesPerSplit, minLeafPopulation, bagFraction, maxNodes, seed\n",
    "#         rf_classifier = ee.Classifier.smileRandomForest(100).setOutputMode('REGRESSION').train(features=training_data, \\\n",
    "#                                                                                                classProperty=responseBand, \\\n",
    "#                                                                                                inputProperties=input_bands)\n",
    "#         rf_classified = unclassified.select(bands).classify(rf_classifier, 'ALR_'+responseBand).clip(mapBounds)\n",
    "        \n",
    "#         return img.addBands(rf_classified)\n",
    "#     return wrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ALR function for server side, one module testing only ,not through\n",
    "# def func2_ALR(responseBand):\n",
    "#     def wrap(img):\n",
    "#         #inputImage = ee.Image(cloud_folder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "#         inputImage = ee.Image(img).select(1,2,3,7,22,23,27,28,29,30,31,32)       \n",
    "#         inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'QA60', 'date', 'estimate'+outputName, 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "#         # inputImage = inputImage.rename(inputImage_bands)\n",
    "     \n",
    "        \n",
    "#         input_VI_definition = ee.List([\n",
    "#                                        \"GI      = b('B3')/b('B4')\",                               \n",
    "#                                        \"SGI     = b('B8')/b('B4')\",                                \n",
    "#                                        \"GVI     = (b('B8')/b('B3'))-1\",                             \n",
    "#                                        \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",                                \n",
    "#                                        \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "#                                        \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",                                \n",
    "#                                        \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",                                 \n",
    "#                                        \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "#                                        \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "#                                        \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "#                                        \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",                            \n",
    "#                                        \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",                                \n",
    "#                                        \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])\n",
    "        \n",
    "\n",
    "#         # names of bands to pass to ALR method (excluding metadata and other non-spectral bands)\n",
    "#         input_bandNames = ['B2', 'B3', 'B4', 'B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI']\n",
    "\n",
    "#         # def format_image(image, image_bands, response_band, VI_definition):\n",
    "#         # image = ee.Image(image)\n",
    "#         # image_bands = ee.List(image_bands)\n",
    "#         # response_band = ee.String(response_band)\n",
    "#         # VI_definition = ee.List(VI_definition)\n",
    "\n",
    "#         # image_bands specifices a list of the names of the bands used in defining the expressions for VIs in VI_definition\n",
    "#         image = inputImage.rename(inputImage_bands).toDouble()\n",
    "\n",
    "#         # Generate an ImageCollection from a list of expressions defining a set of VIs using the bands available in the image\n",
    "#         # VIimageCollection = ee.ImageCollection(input_VI_definition.map(lambda expr:image.expression(expr)))\n",
    "#         VIimageCollection = ee.ImageCollection(input_VI_definition.map(lambda expr:image.expression(expr)))\n",
    "#         # VIimage = input_VI_definition.map(lambda expr:ee.Image(image).expression(expr))\n",
    "#         # VIimageCollection =   ee.ImageCollection.fromImages(ee.List(input_VI_definition.map(lambda expr:image.expression(expr))))\n",
    "     \n",
    "#         # VIimageCollection = input_VI_definition.map(lambda expr:image.expression(expr))\n",
    "#         VIimage = VIimageCollection.toBands().regexpRename(\"[0-9]+_\", \"\")\n",
    "        \n",
    "\n",
    "#         # Reorder the bands in the image so the response band is the first band in the image\n",
    "#         # feature_bands = inputImage_bands.remove(ee.String(responseBand))\n",
    "\n",
    "#         # return ee.Image(image.select(ee.String(responseBand)).addBands(VIimage).addBands(image.select(feature_bands)))\n",
    "#         # return ee.Image(image.select(ee.String(responseBand)).addBands(image.select(feature_bands)))\n",
    "#         # inputImage= ee.Image(image.select(ee.String(responseBand)).addBands(VIimage).addBands(image.select(feature_bands)))\n",
    "#         # return ee.Image(VIimage)\n",
    "#         return img.addBands(VIimage)\n",
    "\n",
    "\n",
    "#         # inputImage = format_image(inputImage, inputImage_bands, responseBand, input_VI_definition)\n",
    "#         # return inputImage\n",
    "#     return wrap  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 1 – SL2P/SL2P10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SL2P Original (create image and export to EE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # parse the networks\n",
    "# colName = 'COPERNICUS/S2_SR'\n",
    "# colOptions = COLLECTION_OPTIONS[colName]\n",
    "# netOptions = VIS_OPTIONS[outputName][colName]\n",
    "# numNets = ee.Number(ee.Feature((COLLECTION_OPTIONS[colName][\"Network_Ind\"]).first()).propertyNames().remove('Feature Index').remove('system:index').remove('lon').size())\n",
    "# SL2P = ee.List.sequence(1,ee.Number(COLLECTION_OPTIONS[colName][\"numVariables\"]),1).map(lambda netNum: wn.makeNetVars(COLLECTION_OPTIONS[colName][\"Collection_SL2P\"],numNets,netNum));\n",
    "# errorsSL2P = ee.List.sequence(1,ee.Number(COLLECTION_OPTIONS[colName][\"numVariables\"]),1).map(lambda netNum: wn.makeNetVars(COLLECTION_OPTIONS[colName][\"Collection_SL2Perrors\"],numNets,netNum));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # filter collection and add ancillary bands\n",
    "\n",
    "# input_collection = ee.ImageCollection(testImage) \\\n",
    "#                      .map(lambda image: ib.addDate(image)) \\\n",
    "#                      .map(lambda image: image.clip(mapBounds)) \\\n",
    "#                      .map(lambda image: ib.s2MaskClear(image)) \\\n",
    "#                      .map(lambda image: ib.s2MaskLand(image)) \\\n",
    "#                      .map(lambda image: ib.addS2Geometry(colOptions, image))\n",
    "\n",
    "# # get partition used to select network\n",
    "# partition = (COLLECTION_OPTIONS[colName][\"partition\"]).filterBounds(mapBounds).mosaic().clip(mapBounds).rename('partition')\n",
    "\n",
    "# # pre process input imagery and flag invalid inputs\n",
    "# scaled_input_collection = input_collection.map(lambda image: ib.scaleBands(netOptions[\"inputBands\"],netOptions[\"inputScaling\"],image)) \\\n",
    "#                                           .map(lambda image: ib.invalidInput(COLLECTION_OPTIONS[colName][\"sl2pDomain\"],netOptions[\"inputBands\"],image))\n",
    "\n",
    "# # apply networks to produce mapped parameters\n",
    "# estimateSL2P = scaled_input_collection.map(lambda image: wn.wrapperNNets(SL2P, partition, netOptions, COLLECTION_OPTIONS[colName], \"estimate\", image, outputName))\n",
    "# uncertaintySL2P = scaled_input_collection.map(lambda image: wn.wrapperNNets(errorsSL2P, partition, netOptions, COLLECTION_OPTIONS[colName], \"error\", image, outputName))\n",
    "\n",
    "# # scale and offset mapped parameter bands\n",
    "# estimateSL2P = estimateSL2P.map(lambda image: image.addBands(image.select(\"estimate\"+outputName).multiply(ee.Image.constant(outputScale)).add(ee.Image.constant(outputOffset)), overwrite=True))\n",
    "# uncertaintySL2P = uncertaintySL2P.map(lambda image: image.addBands(image.select(\"error\"+outputName).multiply(ee.Image.constant(outputScale)).add(ee.Image.constant(outputOffset)), overwrite=True))\n",
    "\n",
    "# # produce final export collection\n",
    "# export_collection = input_collection.combine(estimateSL2P).combine(uncertaintySL2P)\n",
    "    \n",
    "# image_output_names = ([name +\"_\"+siteSelect +\"_\"+outputName for name in export_collection.toList(export_collection.size()).map(lambda image: ee.Image(image).id()).getInfo()])\n",
    "# ee_func.displayImage(export_collection.mosaic().select('estimate'+outputName),0+outputOffset,10*outputScale+outputOffset, mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter collection and add ancillary bands\n",
    "# # test=ee.ImageCollection('COPERNICUS/S2_SR').filterDate('2020-08-01','2020-08-30').filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',20)).filterBounds(mapBounds).filterMetadata('MGRS_TILE', 'equals', 'T11UNA')\n",
    "# test=ee.ImageCollection('COPERNICUS/S2_SR').filterDate('2021-07-01','2021-07-20').filterMetadata('MGRS_TILE','equals','11UNA').filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',20))\n",
    "\n",
    "# input_collection = ee.ImageCollection(test) \\\n",
    "#                      .map(lambda image: ib.addDate(image)) \\\n",
    "#                      .map(lambda image: image.clip(mapBounds)) \\\n",
    "#                      .map(lambda image: ib.s2MaskClear(image)) \\\n",
    "#                      .map(lambda image: ib.s2MaskLand(image)) \\\n",
    "#                      .map(lambda image: ib.addS2Geometry(colOptions, image))\n",
    "\n",
    "# # get partition used to select network\n",
    "# partition = (COLLECTION_OPTIONS[colName][\"partition\"]).filterBounds(mapBounds).mosaic().clip(mapBounds).rename('partition')\n",
    "\n",
    "# # pre process input imagery and flag invalid inputs\n",
    "# scaled_input_collection = input_collection.map(lambda image: ib.scaleBands(netOptions[\"inputBands\"],netOptions[\"inputScaling\"],image)) \\\n",
    "#                                           .map(lambda image: ib.invalidInput(COLLECTION_OPTIONS[colName][\"sl2pDomain\"],netOptions[\"inputBands\"],image))\n",
    "\n",
    "# # apply networks to produce mapped parameters\n",
    "# estimateSL2P = scaled_input_collection.map(lambda image: wn.wrapperNNets(SL2P, partition, netOptions, COLLECTION_OPTIONS[colName], \"estimate\", image, outputName))\n",
    "# uncertaintySL2P = scaled_input_collection.map(lambda image: wn.wrapperNNets(errorsSL2P, partition, netOptions, COLLECTION_OPTIONS[colName], \"error\", image, outputName))\n",
    "\n",
    "# # scale and offset mapped parameter bands\n",
    "# estimateSL2P = estimateSL2P.map(lambda image: image.addBands(image.select(\"estimate\"+outputName).multiply(ee.Image.constant(outputScale)).add(ee.Image.constant(outputOffset)), overwrite=True))\n",
    "# uncertaintySL2P = uncertaintySL2P.map(lambda image: image.addBands(image.select(\"error\"+outputName).multiply(ee.Image.constant(outputScale)).add(ee.Image.constant(outputOffset)), overwrite=True))\n",
    "\n",
    "# # produce final export collection\n",
    "# export_collection = input_collection.combine(estimateSL2P).combine(uncertaintySL2P)\n",
    "    \n",
    "# image_output_names = ([name +\"_\"+siteSelect +\"_\"+outputName for name in export_collection.toList(export_collection.size()).map(lambda image: ee.Image(image).id()).getInfo()])\n",
    "# # ee_func.displayImage(export_collection.mosaic().select('estimate'+outputName),0+outputOffset,10*outputScale+outputOffset, mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export tasks to Earth Engine\n",
    "# export_task = ee_func.export_collection_to_gee(collection=export_collection,\n",
    "#                                                num_images=export_collection.size().getInfo(),\n",
    "#                                                # image_names=[siteSelect+'_'+outputName+'_SL2P'],\n",
    "#                                                image_names = image_output_names,\n",
    "#                                                scale=10,\n",
    "#                                                # asset_folder='users/kateharvey/SL2P_images',\n",
    "#                                                # asset_folder=assetfolder,\n",
    "#                                                asset_folder=cloud_folder,\n",
    "#                                                data_type=exportDataType,\n",
    "#                                                max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(export_task)\n",
    "# ee_func.check_ee_tasks(export_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (image_output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (export_collection.size().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (export_collection.first().bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (export_collection.getInfo())\n",
    "# print (image_output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proj10m =export_collection.first().select('date').projection();\n",
    "# print (proj10m.getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proj10m =export_collection.first().select('B3').projection();\n",
    "# print (proj10m.getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (proj10m.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export_collection10m=ee.ImageCollection(export_collection).map(lambda image: image.reduceResolution(\n",
    "#       reducer= ee.Reducer.mean(),\n",
    "#       maxPixels= 1024\n",
    "#     )\n",
    "#     .reproject(\n",
    "#       crs= proj10m\n",
    "#     )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proj_test =export_collection10m.first().select('B1').projection();\n",
    "# print (proj_test.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (export_collection.first())\n",
    "# results=ee.ImageCollection(export_collection).map(func_ALR(responseBand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results=ee.ImageCollection(export_collection).map(lambda img:func_ALR(img, responseBand, outputName, mapBounds))\n",
    "# results=ee.ImageCollection(export_collection).map(func2_ALR(responseBand, outputName, mapBounds))\n",
    "# results=ee.ImageCollection(export_collection).map(func1_ALR(responseBand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (ee.List(results.toList(1)).get(0))\n",
    "# print (ee.ImageCollection(results).first().bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # export tasks to Earth Engine\n",
    "# export_task = ee_func.export_collection_to_gee(collection=export_collection,\n",
    "#                                                num_images=1,\n",
    "#                                                # image_names=[siteSelect+'_'+outputName+'_SL2P'],\n",
    "#                                                image_names = image_output_names,\n",
    "#                                                scale=10,\n",
    "#                                                # asset_folder='users/kateharvey/SL2P_images',\n",
    "#                                                # asset_folder=assetfolder,\n",
    "#                                                asset_folder=cloud_folder,\n",
    "#                                                data_type=exportDataType,\n",
    "#                                                max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # // Export the image to Cloud Storage.\n",
    "# task = ee.batch.Export.image.toCloudStorage(\n",
    "#   image= export_collection.first(),\n",
    "#   description= 'imageToCloudExample',\n",
    "#   bucket= 'eealr',\n",
    "#   fileNamePrefix= 'Wabush_LAI',\n",
    "#   # crs: projection.crs,\n",
    "#   # crsTransform: projection.transform,\n",
    "#   region= mapBounds\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (export_collection.getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SL2P10 (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # parse the networks\n",
    "# colName = 'COPERNICUS/S2_SR_10m'\n",
    "# colOptions = COLLECTION_OPTIONS[colName]\n",
    "# netOptions = VIS_OPTIONS[outputName][colName]\n",
    "# numNets = ee.Number(ee.Feature((COLLECTION_OPTIONS[colName][\"Network_Ind\"]).first()).propertyNames().remove('Feature Index').remove('system:index').remove('lon').size())\n",
    "# SL2P = ee.List.sequence(1,ee.Number(COLLECTION_OPTIONS[colName][\"numVariables\"]),1).map(lambda netNum: wn.makeNetVars(COLLECTION_OPTIONS[colName][\"Collection_SL2P\"],numNets,netNum));\n",
    "# errorsSL2P = ee.List.sequence(1,ee.Number(COLLECTION_OPTIONS[colName][\"numVariables\"]),1).map(lambda netNum: wn.makeNetVars(COLLECTION_OPTIONS[colName][\"Collection_SL2Perrors\"],numNets,netNum));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # performs same procedure as above, using SL2P10 network\n",
    "# # applies algorithm to 10 m bands ; generates a 10 m map\n",
    "\n",
    "# # filter collection and add ancillary bands\n",
    "# input_collection_10m = ee.ImageCollection(testImage) \\\n",
    "#                      .map(lambda image: ib.addDate(image)) \\\n",
    "#                      .map(lambda image: image.clip(mapBounds)) \\\n",
    "#                      .map(lambda image: ib.s2MaskClear(image)) \\\n",
    "#                      .map(lambda image: ib.s2MaskLand(image)) \\\n",
    "#                      .map(lambda image: ib.addS2Geometry(colOptions, image))\n",
    "\n",
    "# # get partition used to select network\n",
    "# partition = (COLLECTION_OPTIONS[colName][\"partition\"]).filterBounds(mapBounds).mosaic().clip(mapBounds).rename('partition')\n",
    "\n",
    "# # pre process input imagery and flag invalid inputs\n",
    "# scaled_input_collection_10m = input_collection_10m.map(lambda image: ib.s2MaskLand(image)) \\\n",
    "#                                                   .map(lambda image: ib.scaleBands(netOptions[\"inputBands\"],netOptions[\"inputScaling\"],image)) \\\n",
    "#                                                   .map(lambda image: ib.invalidInput(COLLECTION_OPTIONS[colName][\"sl2pDomain\"],netOptions[\"inputBands\"],image))\n",
    "\n",
    "# # apply networks to produce mapped parameters\n",
    "# estimateSL2P_10m = scaled_input_collection_10m.map(lambda image: wn.wrapperNNets(SL2P, partition, netOptions, COLLECTION_OPTIONS[colName], \"estimate\", image, outputName))\n",
    "# uncertaintySL2P_10m = scaled_input_collection_10m.map(lambda image: wn.wrapperNNets(errorsSL2P, partition, netOptions, COLLECTION_OPTIONS[colName], \"error\", image, outputName))\n",
    "\n",
    "# # scale and offset mapped parameter bands\n",
    "# estimateSL2P_10m = estimateSL2P_10m.map(lambda image: image.addBands(image.select(\"estimate\"+outputName) \\\n",
    "#                                                              .multiply(ee.Image.constant(outputScale)) \\\n",
    "#                                                              .add(ee.Image.constant(outputOffset)), overwrite=True))\n",
    "# uncertaintySL2P_10m = uncertaintySL2P_10m.map(lambda image: image.addBands(image.select(\"error\"+outputName) \\\n",
    "#                                                                    .multiply(ee.Image.constant(outputScale)) \\\n",
    "#                                                                    .add(ee.Image.constant(outputOffset)), overwrite=True))\n",
    "\n",
    "\n",
    "# # produce final export collection\n",
    "# export_collection_10m = input_collection_10m.combine(estimateSL2P_10m).combine(uncertaintySL2P_10m)\n",
    "\n",
    "# image_output_names_10m = ([name+\"_\"+siteSelect+\"_\"+outputName+\"_10m\" for name in export_collection_10m.toList(export_collection_10m.size()).map(lambda image: ee.Image(image).id()).getInfo()])\n",
    "\n",
    "# ee_func.displayImage(export_collection_10m.mosaic().select('estimate'+outputName),0+outputOffset,10*outputScale+outputOffset, mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # export tasks to Earth Engine\n",
    "# export_task_10m = ee_func.export_collection_to_gee(collection=export_collection_10m,\n",
    "#                                                    num_images=1,\n",
    "#                                                    # image_names=[siteSelect+'_'+outputName+'_SL2P10'],\n",
    "#                                                    image_names = image_output_names_10m,\n",
    "#                                                    scale=10,\n",
    "#                                                    # asset_folder='users/kateharvey/SL2P10_images',\n",
    "#                                                    # asset_folder=assetfolder,\n",
    "#                                                    asset_folder=cloud_folder,\n",
    "#                                                    data_type=exportDataType,\n",
    "#                                                    max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALR function for estimation, one image works\n",
    "def func3_ALR(temp_image, responsedBand, outputName, mapBounds):\n",
    "    #inputImage = ee.Image(cloud_folder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "    inputImage = ee.Image(temp_image).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "    inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'QA60', 'date', 'estimate'+outputName, 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "    inputImage = inputImage.rename(inputImage_bands)\n",
    "    input_VI_definition = ee.List([\n",
    "                                   \"GI      = b('B3')/b('B4')\",                               \n",
    "                                   \"SGI     = b('B8')/b('B4')\",                                \n",
    "                                   \"GVI     = (b('B8')/b('B3'))-1\",                             \n",
    "                                   \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",                                \n",
    "                                   \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "                                   \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",                                \n",
    "                                   \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",                                 \n",
    "                                   \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "                                   \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "                                   \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "                                   \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",                            \n",
    "                                   \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",                                \n",
    "                                   \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])\n",
    "\n",
    "    # names of bands to pass to ALR method (excluding metadata and other non-spectral bands)\n",
    "    input_bandNames = ['B2', 'B3', 'B4', 'B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI']\n",
    "    \n",
    "    def format_image(image, image_bands, response_band, VI_definition):\n",
    "        image = ee.Image(image)\n",
    "        image_bands = ee.List(image_bands)\n",
    "        response_band = ee.String(response_band)\n",
    "        VI_definition = ee.List(VI_definition)\n",
    "        \n",
    "        # image_bands specifices a list of the names of the bands used in defining the expressions for VIs in VI_definition\n",
    "        image = image.rename(image_bands).toDouble()\n",
    "        \n",
    "        # Generate an ImageCollection from a list of expressions defining a set of VIs using the bands available in the image\n",
    "        VIimageCollection = ee.ImageCollection(VI_definition.map(lambda expr: image.expression(expr)))\n",
    "        VIimage = VIimageCollection.toBands().regexpRename(\"[0-9]+_\", \"\")\n",
    "        \n",
    "        # Reorder the bands in the image so the response band is the first band in the image\n",
    "        feature_bands = image_bands.remove(response_band)\n",
    "        \n",
    "        return ee.Image(image.select(response_band).addBands(VIimage).addBands(image.select(feature_bands)))\n",
    "\n",
    "    \n",
    "    inputImage = format_image(inputImage, inputImage_bands, responseBand, input_VI_definition)\n",
    "    \n",
    "    def scale_image(image, response_band):\n",
    "        image = ee.Image(image)\n",
    "        response_band = ee.String(response_band)\n",
    "        \n",
    "        def get_num_pixels(image):\n",
    "    \n",
    "            # get image height\n",
    "            def get_height(image):\n",
    "                height = image.getInfo()[\"bands\"][0][\"dimensions\"][0]\n",
    "                return height\n",
    "\n",
    "            # get image width\n",
    "            def get_width(image):\n",
    "                width = image.getInfo()[\"bands\"][0][\"dimensions\"][1]\n",
    "                return width\n",
    "\n",
    "            image_height = get_height(image)\n",
    "            image_width = get_width(image)\n",
    "            image_pixels = image_height*image_width\n",
    "\n",
    "            return image_pixels\n",
    "        \n",
    "        image_pixels = ee.Number(get_num_pixels(image))\n",
    "        \n",
    "        # Set up lists containing the input/feature bands in the image\n",
    "        bandList = image.bandNames()\n",
    "        featureList = bandList.remove(response_band)\n",
    "        num_bands = bandList.length()\n",
    "        num_features = featureList.length()\n",
    "        \n",
    "        # We will be using the reduceRegion() function on images from Earth Engine, \n",
    "        # which will process up to a specified number of pixels from the image to generate the outputs of the reducer\n",
    "        max_pixels = image_pixels.min(10000000)\n",
    "        # best_effort = ee.Algorithms.If(image_pixels.gt(max_pixels), True, False)\n",
    "        \n",
    "        # Set default projection and scale using the response band\n",
    "        defaultScale = image.select(response_band).projection().nominalScale()\n",
    "        defaultCrs = image.select(response_band).projection().crs()\n",
    "        image = image.setDefaultProjection(crs=defaultCrs, scale=defaultScale)\n",
    "        \n",
    "        # Center all of the bands in the image for LARs\n",
    "        # We will centre the sampled data later as well as reduceRegion() is not precise enough\n",
    "        meanImage = image.subtract(image.reduceRegion(reducer=ee.Reducer.mean(), \\\n",
    "                                    scale=defaultScale, bestEffort=True, maxPixels=max_pixels).toImage(bandList))\n",
    "        \n",
    "        # Separate the image into features (X) and response (y) as we need to standardize the input features\n",
    "        X = meanImage.select(featureList)\n",
    "        y = meanImage.select(response_band)\n",
    "        \n",
    "        # Standardize the input features\n",
    "        X = X.divide(X.reduceRegion(reducer=ee.Reducer.stdDev(), bestEffort=True, maxPixels=max_pixels).toImage(featureList))\n",
    "        \n",
    "        return X.addBands(y)\n",
    "\n",
    "    \n",
    "    scaledImage = scale_image(inputImage, responseBand)\n",
    "    \n",
    "    \n",
    "    def ee_LARS(input_image, input_bandNames, response_bandName, num_nonzero_coefficients, num_samples):\n",
    "        image = ee.Image(input_image)\n",
    "        feature_list = ee.List(input_bandNames)\n",
    "        response_band = ee.String(response_bandName)\n",
    "        full_band_list = ee.List(feature_list).add(response_band)\n",
    "        num_nonzero_coefficients = ee.Number(num_nonzero_coefficients)\n",
    "        num_samples = ee.Number(num_samples)\n",
    "        def get_num_pixels(image):\n",
    "    \n",
    "            # get image height\n",
    "            def get_height(image):\n",
    "                height = image.getInfo()[\"bands\"][0][\"dimensions\"][0]\n",
    "                return height\n",
    "\n",
    "            # get image width\n",
    "            def get_width(image):\n",
    "                width = image.getInfo()[\"bands\"][0][\"dimensions\"][1]\n",
    "                return width\n",
    "\n",
    "            image_height = get_height(image)\n",
    "            image_width = get_width(image)\n",
    "            image_pixels = image_height*image_width\n",
    "\n",
    "            return image_pixels\n",
    "        image_pixels = ee.Number(get_num_pixels(image))\n",
    "        \n",
    "        # Randomly sample pixels in the image at native resolution into a FeatureCollection\n",
    "        input_collection = image.sample(numPixels=num_samples.min(image_pixels))\n",
    "        n = input_collection.size()\n",
    "        m = feature_list.length()\n",
    "        \n",
    "        # Use an aggregate array function over the FeatureCollection and map the function over each feature in the band list\n",
    "        # to generate a dictionary of all of the samples retrieved\n",
    "        inputs = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_array(feature)))\n",
    "        \n",
    "        # Although we may call our scale_image function on the input image, the reduceRegion() function used to determine the mean\n",
    "        # and standard deviation of each band in the image over the entire region is not precise enough over a large image\n",
    "        # so we must recenter all of the bands in the image and now we can also normalize (L2 norm) each input feature as required\n",
    "        # by the LARs algorithm\n",
    "        \n",
    "        # Use an aggregate_mean function over the feature collection to get the mean of each band\n",
    "        input_means = ee.Dictionary.fromLists(full_band_list, full_band_list.map(lambda feature: input_collection.aggregate_mean(feature)))\n",
    "\n",
    "        def centre_inputs(key, value):\n",
    "            key_mean = input_means.getNumber(key)\n",
    "            return ee.List(value).map(lambda sample: ee.Number(sample).subtract(key_mean))\n",
    "        \n",
    "        \n",
    "        # Center bands by mapping over the list of features and then a subtracting over the list of samples for each band\n",
    "        inputs = inputs.map(centre_inputs)\n",
    "\n",
    "        # Separate the response variable samples into its own vector\n",
    "        y = inputs.toArray([response_band]).reshape([-1,1])\n",
    "\n",
    "        # Remove response band from the feature collection by selecting only bands in the feature list\n",
    "        inputs = inputs.select(feature_list)\n",
    "        \n",
    "        # Generate a dictionary of all of the L2 norms of the input features using a custom mapped function\n",
    "        input_norms = inputs.map(lambda key, value: ee.Number(ee.List(value).map(lambda sample: ee.Number(sample).pow(2)).reduce(ee.Reducer.sum())).pow(0.5))\n",
    "\n",
    "        def norm_inputs(key, value):\n",
    "            key_norm = input_norms.getNumber(key)\n",
    "            return ee.List(value).map(lambda sample: ee.Number(sample).divide(key_norm))\n",
    "        \n",
    "        # Normalize all of the features by mapping a function over the list of features\n",
    "        # and then map a division over the list of all of the samples of the feature\n",
    "        inputs = inputs.map(norm_inputs)\n",
    "        \n",
    "        # Generate the array of samples using the dictionary\n",
    "        X = inputs.toArray(feature_list).transpose()\n",
    "\n",
    "        # Find the first best predictor of the response to initialize the main LARs loop\n",
    "        initial_prediction = ee.Array(ee.List.repeat([0], n))\n",
    "        c = X.transpose().matrixMultiply(y.subtract(initial_prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_maxLoc = c_abs.project([0]).argmax()\n",
    "        add_feature = C_maxLoc.getNumber(0)\n",
    "        A = ee.List([add_feature])\n",
    "        \n",
    "        # Create a dicitionary of initial inputs to pass into the main LARs iterative loop\n",
    "        # The iterate function in Earth Engine processes each iteration as a tree of iterations with no access to any variables\n",
    "        # from previous iterations (only those that are passed to the next iteration)\n",
    "        # so we must pass both the current prediction and the active set of features (with non-zero coefficients), A\n",
    "        initial_inputs = ee.Dictionary({'prediction': initial_prediction, 'A': A})\n",
    "\n",
    "        def LARs_regression(iteration, inputs):\n",
    "            inputs = ee.Dictionary(inputs)\n",
    "\n",
    "            # Find the active set of features, A (predictors with non-zero coefficients)\n",
    "            A = ee.List(inputs.get('A'))\n",
    "            # A_list is an array used to mask the full array of input samples and the correlation vector\n",
    "            A_list = ee.Array(ee.List.sequence(0, m.subtract(1))\\\n",
    "                              .map(lambda index: A.contains(index)).replaceAll(False, 0).replaceAll(True, 1)).reshape([-1,1])\n",
    "\n",
    "            # The following matrix algebra determines the next most correlated variable, or the next best predictor considering the\n",
    "            # current features in the active set, A, as well as the magnitude to adjust the prediction vector to ensure all of the\n",
    "            # features in the active set are equally correlated to response vector\n",
    "            prediction = inputs.getArray('prediction')\n",
    "            c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "            c_abs = c.abs()\n",
    "            C_max = c_abs.get(c_abs.argmax())\n",
    "            s_A = c.divide(c_abs).mask(A_list)\n",
    "            X_A = X.mask(A_list.transpose())\n",
    "            G_Ai = X_A.transpose().matrixMultiply(X_A).matrixInverse()\n",
    "            G1 = G_Ai.matrixMultiply(s_A)\n",
    "            A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "            w_A = G1.multiply(A_A)\n",
    "            u_A = X_A.matrixMultiply(w_A)\n",
    "            a = X.transpose().matrixMultiply(u_A)\n",
    "            a = a.project([0])\n",
    "            c = c.project([0])\n",
    "\n",
    "            def compute_gammaArray(index_j):\n",
    "                minus_j = C_max.subtract(c.get([index_j])).divide(A_A.subtract(a.get([index_j])))\n",
    "                plus_j = C_max.add(c.get([index_j])).divide(A_A.add(a.get([index_j])))\n",
    "                return ee.List([minus_j, plus_j]).filter(ee.Filter.gte('item', 0)).reduce(ee.Reducer.min())\n",
    "\n",
    "            A_c = ee.List.sequence(0, m.subtract(1)).removeAll(A)\n",
    "            gammaArray = A_c.map(compute_gammaArray)\n",
    "            gamma = gammaArray.reduce(ee.Reducer.min())\n",
    "            min_location = gammaArray.indexOf(gamma)\n",
    "            add_feature = A_c.getNumber(min_location)\n",
    "\n",
    "            # Update active set of variables with next best predictor from non-active set and update prediction vector\n",
    "            A = A.add(add_feature)\n",
    "            prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "            return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "\n",
    "        # The final iteration of LARs (if selecting all input variables) requires a different method to determine magnitude for\n",
    "        # adjusting the magnitude of the prediction vector, as the regular LARs iteration relies on variables in non-active set\n",
    "        # In the final iteration there will be no variables in the non-active set, so the method will not work\n",
    "        def LARs_final_iteration(iteration, inputs):\n",
    "            inputs = ee.Dictionary(inputs)\n",
    "            A = ee.List(inputs.get('A'))\n",
    "\n",
    "            prediction = inputs.getArray('prediction')\n",
    "            c = X.transpose().matrixMultiply(y.subtract(prediction))\n",
    "            c_abs = c.abs()\n",
    "            C_max = c_abs.get(c_abs.argmax())        \n",
    "\n",
    "            s_A = c.divide(c_abs)\n",
    "            G_Ai = X.transpose().matrixMultiply(X).matrixInverse()\n",
    "            G1 = G_Ai.matrixMultiply(s_A)\n",
    "            A_A = s_A.project([0]).dotProduct(G1.project([0])).pow(-0.5)\n",
    "            w_A = G1.multiply(A_A)\n",
    "            u_A = X.matrixMultiply(w_A)\n",
    "\n",
    "            gamma = C_max.divide(A_A)\n",
    "            prediction = prediction.add(u_A.multiply(gamma))\n",
    "\n",
    "            return ee.Dictionary({'prediction': prediction, 'A': A})\n",
    "\n",
    "        # Actually carrying out the iterations by iterating over a placeholder list (sequence from 1 to the number of non-zero\n",
    "        # variables that the user wishes to select as predictors for the response)\n",
    "        iterations = ee.List.sequence(1, m.subtract(1).min(num_nonzero_coefficients))\n",
    "        penultimate_outputs = iterations.iterate(LARs_regression, initial_inputs)\n",
    "        final_outputs = ee.Dictionary(ee.Algorithms.If(num_nonzero_coefficients.gte(m), \\\n",
    "                                LARs_final_iteration(m, penultimate_outputs), penultimate_outputs))\n",
    "        \n",
    "        final_prediction = final_outputs.getArray('prediction')\n",
    "\n",
    "        A = ee.List(final_outputs.get('A'))\n",
    "\n",
    "        feature_path = A.slice(0, num_nonzero_coefficients).map(lambda index: feature_list.getString(index))\n",
    "\n",
    "        # The code snippet below is able to extract the exact coefficients on all of the selected features, but is commented out\n",
    "        # as it adds computational complexity that takes up unnecessary memory on the Google Earth Engine virtual machine since we\n",
    "        # are only using LARs as a feature selection algorithm\n",
    "\n",
    "    #     coefficients = X.matrixSolve(final_prediction).project([0])\\\n",
    "    #                               .toList().map(lambda num: ee.Algorithms.If(ee.Number(num).abs().lt(0.001), 0, num))\n",
    "    #     print('Coefficients')\n",
    "    #     coeff = ee.Dictionary.fromLists(featureList, coefficients).getInfo()\n",
    "    #     ordered_coeff = OrderedDict()\n",
    "    #     var_path = feature_path.cat(featureList.removeAll(feature_path)).getInfo()\n",
    "    #     for key in var_path:\n",
    "    #         ordered_coeff[key] = coeff[key]\n",
    "    #     print(json.dumps(ordered_coeff, indent=1))\n",
    "        \n",
    "        # print('selected features: ', feature_path.getInfo())\n",
    "        \n",
    "        return feature_path\n",
    "\n",
    "    \n",
    "    select_features = ee_LARS(scaledImage, input_bandNames, responseBand, 5, 50000)\n",
    "    #unclassified = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_VI')\n",
    "    unclassified = ee.Image(inputImage)\n",
    "    bands = ee.List([responseBand, 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI',\n",
    "                     'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI', 'B2', 'B3', 'B4', 'B8',\n",
    "                     'QA60', 'date', 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "    unclassified = unclassified.rename(bands)\n",
    "\n",
    "    # prediction bands (equivalent to select_features, with responseBand)\n",
    "    bands = select_features\n",
    "    input_bands = select_features.add(responseBand)\n",
    "    training_data = ee.FeatureCollection(unclassified.sample(numPixels=1000, seed=1).select(input_bands))\n",
    "    # implement regression tree with Random Forest algorithm\n",
    "    # optional parameters for smileRandomForest(): variablesPerSplit, minLeafPopulation, bagFraction, maxNodes, seed\n",
    "    rf_classifier = ee.Classifier.smileRandomForest(100).setOutputMode('REGRESSION').train(features=training_data,\n",
    "                                                                                           classProperty=responseBand,\n",
    "                                                                                           inputProperties=input_bands)\n",
    "    rf_classified = unclassified.select(bands).classify(rf_classifier, 'ALR_'+responseBand).clip(mapBounds)\n",
    "    return temp_image.addBands(rf_classified)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tem_image = ee.Image(cloud_folder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "# tem_image = ee.Image(cloud_folder+'/'+image_output_names[0])\n",
    "# output_result=func_ALR(tem_image, responseBand, outputName, mapBounds)\n",
    "# print (tem_image.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_result=func3_ALR(tem_image, responseBand, outputName, mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (output_result.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing- I used the module'SL2P Original (create image and export to EE)' to genernate three images after running  S2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=ee.Image('projects/ccmeo-ag-000008/assets/ALR/20210701T185921_20210701T185918_T11UNA_FoxCreek_LAI')\n",
    "img2=ee.Image('projects/ccmeo-ag-000008/assets/ALR/20210708T184921_20210708T185756_T11UNA_FoxCreek_LAI')\n",
    "img3=ee.Image('projects/ccmeo-ag-000008/assets/ALR/20210805T185919_20210805T190825_T11UNA_FoxCreek_LAI')\n",
    "# img3=ee.Image('projects/ccmeo-ag-000008/assets/ALR/20210825T185919_20210825T190431_T11UNA_FoxCreek_LAI')\n",
    "# imgcol=ee.ImageCollection.fromImages([img1,img2, img3])\n",
    "imgcol=ee.ImageCollection([img1,img2, img3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'WVP', 'SCL', 'TCI_R', 'TCI_G', 'TCI_B', 'MSK_CLDPRB', 'MSK_SNWPRB', 'QA10', 'QA20', 'QA60', 'date', 'cosVZA', 'cosSZA', 'cosRAA', 'estimateLAI', 'partition', 'networkID', 'errorLAI', 'partition_1', 'networkID_1', 'ALR_estimateLAI']\n"
     ]
    }
   ],
   "source": [
    "output_result1=func3_ALR(img1, responseBand, outputName, mapBounds)\n",
    "print (output_result1.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print (imgcol.size().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# output_result2=ee.ImageCollection(imgcol).map(func2_ALR(responseBand))\n",
    "# print (output_result2.size().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (output_result2.first().bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# output_result3=ee.ImageCollection(imgcol).map(func1_ALR(responseBand, outputName, mapBounds))\n",
    "# print (output_result3.size().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (output_result3.first().bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no more testing after this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display map\n",
    "vis_params = {\n",
    "    'min': 0,\n",
    "    'max': outputScale*outputMax}\n",
    "\n",
    "rf_map = ee.Image(ee_func.displayImage(output_result.select('ALR_estimateLAI'), vis_params['min'], vis_params['max'], mapBounds)).clip(mapBounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 – Active Learning Regularization (LARS Feature Selection)\n",
    "\n",
    "Note: the responseBand from the above step doesn't have a geometry associated with it (only happens after being uploaded to GEE) so the image will have to be defined from existing GEE asset for the remaining steps even though the same image was created as inputImage (should be identical except for geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# order of bands from SL2P output:\n",
    "# 00-11: 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', \n",
    "# 12-19: 'AOT', 'WVP', 'SCL', 'TCI_R', 'TCI_G', 'TCI_B', 'MSK_CLDPRB', 'MSK_SNWPRB', \n",
    "# 20-26: 'QA10', 'QA20', 'QA60', 'date', 'cosVZA', 'cosSZA', 'cosRAA', \n",
    "# 27-32: 'estimateLAI', 'partition', 'networkID', 'errorLAI', 'partition_1', 'networkID_1'\n",
    "\n",
    "# define 10m band input image ; name bands of inputImage and scale response band\n",
    "# inputImage = ee.Image('users/ganghong/ALR/'+siteSelect+'_'+outputName+'_SL2P').select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "# inputImage = ee.Image(assetfolder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "inputImage = ee.Image(cloud_folder+'/'+image_output_names[0]).select(1,2,3,7,22,23,27,28,29,30,31,32)\n",
    "inputImage_bands = ee.List(['B2', 'B3', 'B4', 'B8', 'QA60', 'date', 'estimate'+outputName, 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "inputImage = inputImage.rename(inputImage_bands)\n",
    "# print (inputImage.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (inputImage.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only include VIs that use B2, B3, B4, B8 to create a 10 m product\n",
    "input_VI_definition = ee.List([# \"RAW_B2  = b('B2')\",\n",
    "                             # \"RAW_B3  = b('B3')\",\n",
    "                             # \"RAW_B4  = b('B4')\",\n",
    "                             # \"RAW_B8  = b('B8')\",\n",
    "                               \"GI      = b('B3')/b('B4')\",\n",
    "                             # \"RVI3    = b('B4')/b('B6')\",\n",
    "                             # \"SR3     = b('B5')/b('B4')\",\n",
    "                             # \"GM1     = b('B6')/b('B3')\",\n",
    "                             # \"GM2     = b('B6')/b('B5')\",\n",
    "                             # \"SR2     = b('B7')/b('B3')\",\n",
    "                             # \"PSSR    = b('B7')/b('B4')\",\n",
    "                               \"SGI     = b('B8')/b('B4')\",\n",
    "                             # \"MSI     = b('B11')/b('B7')\",\n",
    "                             # \"II      = b('B11')/b('B12')\",\n",
    "                               \"GVI     = (b('B8')/b('B3'))-1\",\n",
    "                             # \"PSRI    = (b('B4')-b('B3'))/b('B6')\",\n",
    "                               \"NDVI3   = ((b('B8')-b('B4'))/(b('B8')))+b('B4')\",\n",
    "                             # \"SR5     = 1/b('B5')\",\n",
    "                             # \"SR6     = b('B4')/(b('B3')*b('B5'))\",\n",
    "                             # \"SR7     = b('B8')/(b('B3')*b('B5'))\",\n",
    "                             # \"IPVI    = b('B7')/(b('B7')+b('B4'))\",\n",
    "                             # \"ARI     = (1/b('B3'))-(1/b('B5'))\",\n",
    "                             # \"ARI2    = b('B7')*((1/b('B3'))-(1/b('B5')))\",\n",
    "                               \"NDVI    = (b('B8')-b('B4'))/(b('B8')+b('B4'))\",\n",
    "                               \"GNDVI   = (b('B8')-b('B3'))/(b('B8')+b('B3'))\",\n",
    "                             # \"NDWI    = (b('B8')-b('B11'))/(b('B8')+b('B11'))\",\n",
    "                             # \"NDREVI  = (b('B8')-b('B5'))/(b('B8')+b('B5'))\",\n",
    "                               \"NDGI    = (b('B3')-b('B4'))/(b('B3')+b('B4'))\",\n",
    "                             # \"NDI1    = (b('B7')-b('B5'))/(b('B7')-b('B4'))\",\n",
    "                             # \"NDI2    = (b('B8')-b('B5'))/(b('B8')-b('B4'))\",\n",
    "                             # \"RENDVI  = (b('B6')-b('B5'))/(b('B6')+b('B5'))\",\n",
    "                             # \"OSAVI   = (1.16*(b('B7')-b('B4')))/(b('B7')+b('B4')+0.61)\",\n",
    "                             # \"NMDI    = (b('B8')-(b('B11')-b('B12')))/(b('B8')+(b('B11')-b('B12')))\",\n",
    "                             # \"HI      = ((b('B3')-b('B5'))/(b('B3')+b('B5')))-0.5*b('B5')\",\n",
    "                             # \"GVSP    = (-0.283*b('B3') - 0.66*b('B4') + 0.577*b('B6') + 0.388*b('B8'))/(0.433*b('B3') - 0.632*b('B4') + 0.586*b('B6') + 0.264*b('B8A'))\",\n",
    "                             # \"MCARI   = ((b('B5')-b('B4'))-0.2*(b('B5')-b('B3')))*(b('B5')/b('B4'))\",\n",
    "                             # \"TCARI   = 3*((b('B5')-b('B4'))-0.2*(b('B5')-b('B3'))*(b('B5')/b('B4')))\",\n",
    "                               \"EVI     = 2.5*((b('B8')-b('B4'))/(b('B8')+6*b('B4')-7.5*b('B3')+1))\",\n",
    "                               \"EVI2    = 2.5*((b('B8')-b('B4'))/(b('B8')+2.4*b('B4')+1))\",\n",
    "                               \"RDVI    = (b('B8')-b('B4'))/((b('B8')+b('B4'))**0.5)\",\n",
    "                               \"MSR     = ((b('B8')/b('B4'))-1)/((b('B8')/b('B4'))**0.5+1)\",\n",
    "                             # \"MSAVI   = 0.5*(2*b('B7')+1-((2*b('B7')+1)**2-8*(b('B7')-b('B4')))**0.5)\",\n",
    "                               \"MSAVI2  = 0.5*(2*b('B8')+1-((2*b('B8')+1)**2-8*(b('B8')-b('B4')))**0.5)\",\n",
    "                             # \"MCARI2  = (1.5*(2.5*(b('B7')-b('B4'))-1.3*(b('B7')-b('B3'))))/((((2*b('B7')+1)**2)-(6*b('B7')-5*(b('B4')**0.5))-0.5)**0.5)\",\n",
    "                             # \"MTVI2   = (1.5*(1.2*(b('B7')-b('B3'))-2.5*(b('B4')-b('B3'))))/(((2*b('B7')+1)**2-(6*b('B7')-5*b('B4'))-0.5)**0.5)\",\n",
    "                             # \"MSR2    = ((b('B7')/b('B4'))-1)/(((b('B7')/b('B4'))+1)**0.5)\",\n",
    "                               \"NLI     = ((b('B8')**2)-b('B4'))/((b('B8')**2)+b('B4'))\"])\n",
    "\n",
    "# names of bands to pass to ALR method (excluding metadata and other non-spectral bands)\n",
    "input_bandNames = ['B2', 'B3', 'B4', 'B8', 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI', 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format image and generate list of selected features\n",
    "inputImage = alr.format_image(inputImage, inputImage_bands, responseBand, input_VI_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (inputImage.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepares the image to be ingested by the LARS algorithm\n",
    "# returns an image with the response band centred to a mean 0, and the other bands in the image standardized\n",
    "# to a mean 0 and standard deviation 1\n",
    "scaledImage = alr.scale_image(inputImage, responseBand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scaledImage.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ALR to the image and obtain the features selected for the model\n",
    "# parameters: ee_LARS(inputImage, bandNames, responseBand, numFeatures, numSamples)\n",
    "select_features = alr.ee_LARS(scaledImage, input_bandNames, responseBand, 5, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (select_features.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (inputImage.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export VI Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export formatted image to google drive (with added VI bands)\n",
    "# this will be used in the next section to train the regression tree\n",
    "export = ee.ImageCollection(inputImage)\n",
    "\n",
    "export_task = ee_func.export_collection_to_gee(collection=export,\n",
    "                                                 num_images=1,\n",
    "                                                 # image_names=[siteSelect+'_'+outputName+'_VI'],\n",
    "                                                 # asset_folder='users/kateharvey/vi_images',  # replace with EE destination folder                                                 \n",
    "                                                 image_names=[siteSelect+'_'+outputName+'_VI'],\n",
    "                                                 # asset_folder=assetfolder,  # replace with EE destination folder\n",
    "                                                 asset_folder=cloud_folder, \n",
    "                                                 scale=10,\n",
    "                                                 data_type=exportDataType,\n",
    "                                                 max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 2a – Regression Tree (Random Forest and CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and Apply Regression Tree to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_VI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (unclassified.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE IMAGE & FORMAT BANDS\n",
    "# unclassified = ee.Image('users/kateharvey/vi_images/'+siteSelect+'_'+outputName+'_VI')\n",
    "# unclassified = ee.Image(assetfolder+'/'+siteSelect+'_'+outputName+'_VI')\n",
    "# unclassified = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_VI')\n",
    "bands = ee.List([responseBand, 'GI', 'SGI', 'GVI', 'NDVI3', 'NDVI', 'GNDVI', 'NDGI',\n",
    "                 'EVI', 'EVI2', 'RDVI', 'MSR', 'MSAVI2', 'NLI', 'B2', 'B3', 'B4', 'B8',\n",
    "                 'QA60', 'date', 'partition', 'networkID', 'error'+outputName, 'partition_1', 'networkID_1'])\n",
    "unclassified = unclassified.rename(bands)\n",
    "\n",
    "# prediction bands (equivalent to select_features, with responseBand)\n",
    "bands = select_features\n",
    "input_bands = select_features.add(responseBand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (unclassified.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (input_bands.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TRAINING DATASET\n",
    "# Feature Vector (table) used to train regression model (select only prediction bands)\n",
    "training_data = ee.FeatureCollection(unclassified.sample(numPixels=1000, seed=1).select(input_bands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CLASSIFIERS\n",
    "\n",
    "# implement regression tree with Random Forest algorithm\n",
    "# optional parameters for smileRandomForest(): variablesPerSplit, minLeafPopulation, bagFraction, maxNodes, seed\n",
    "rf_classifier = ee.Classifier.smileRandomForest(100).setOutputMode('REGRESSION').train(features=training_data,\n",
    "                                                                                       classProperty=responseBand,\n",
    "                                                                                       inputProperties=input_bands)\n",
    "\n",
    "# implement regression tree with CART (Classification and Regression Tree) algorithm\n",
    "# optional parameters for smileCart(): maxNodes, minLeafPopulation\n",
    "cart_classifier = ee.Classifier.smileCart().setOutputMode('REGRESSION').train(features=training_data,\n",
    "                                                                              classProperty=responseBand,\n",
    "                                                                              inputProperties=input_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFY IMAGE\n",
    "rf_classified = unclassified.select(bands).classify(rf_classifier, 'rf_'+responseBand).clip(mapBounds)\n",
    "cart_classified = unclassified.select(bands).classify(cart_classifier, 'cart_'+responseBand).clip(mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (rf_classified.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Random Forest image to EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display map\n",
    "vis_params = {\n",
    "    'min': 0,\n",
    "    'max': outputScale*outputMax}\n",
    "\n",
    "rf_map = ee.Image(ee_func.displayImage(rf_classified, vis_params['min'], vis_params['max'], mapBounds)).clip(mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export directly to EE:\n",
    "export_task = ee_func.export_collection_to_gee(collection=rf_classified,\n",
    "                                               num_images=1,\n",
    "                                               image_names=[siteSelect+'_'+outputName+'_rf'],\n",
    "                                               scale=10,\n",
    "                                               # asset_folder='users/kateharvey/regression_images',                                            \n",
    "                                               asset_folder=cloud_folder,\n",
    "                                            # asset_folder=assetfolder,\n",
    "                                               data_type=exportDataType,\n",
    "                                               max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export directly to EE:\n",
    "export_task = ee_func.export_collection_to_gee(collection=cart_classified,\n",
    "                                               num_images=1,\n",
    "                                               image_names=[siteSelect+'_'+outputName+'_cart'],\n",
    "                                               scale=10,\n",
    "                                               # asset_folder='users/kateharvey/regression_images',                                            \n",
    "                                               asset_folder=cloud_folder,\n",
    "                                                # asset_folder=assetfolder,\n",
    "                                               data_type=exportDataType,\n",
    "                                               max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_folder = 'projects/ccmeo-ag-000008/assets/ALR'\n",
    "# # export_image = 'projects/google/logistic_demo_image'\n",
    "# # Export directly to EE:\n",
    "# export_task = ee_func.export_collection_to_gee(collection=cart_classified,\n",
    "#                                                num_images=1,\n",
    "#                                                image_names=[siteSelect+'_'+outputName+'_cart1'],\n",
    "#                                                scale=10,\n",
    "#                                                # asset_folder='users/kateharvey/regression_images',                                            \n",
    "#                                                asset_folder=cloud_folder,\n",
    "#                                                data_type=exportDataType,\n",
    "#                                                max_pixels=1e13)\n",
    "\n",
    "# image_task = ee.batch.Export.image.toAsset(\n",
    "#   image = stack, \n",
    "#   description = 'logistic_demo_image', \n",
    "#   assetId = export_image, \n",
    "#   region = GEOMETRY,\n",
    "#   scale = 30,\n",
    "#   maxPixels = 1e10\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sample points for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHECK RESULTS (CROSS-VALIDATION)\n",
    "joined_image = unclassified.select(responseBand).addBands(rf_classified.select('rf_'+responseBand)).addBands(cart_classified.select('cart_'+responseBand))\n",
    "\n",
    "# using same random seed as training_data, get 2000 samples and discard the first 1000, leaving 1000 different samples for cross-validation\n",
    "# this sampling method ensures no overlap between training and testing datasets\n",
    "joined_samples = ee.FeatureCollection(joined_image.sample(numPixels=2000, seed=1).toList(2000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export Feature Collection (for scatter plot comparison in next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_csv = ee.batch.Export.table.toDrive(collection=joined_samples,\n",
    "#                                            description=siteSelect+'_'+outputName+'_regression_tree',\n",
    "#                                            fileFormat='CSV')\n",
    "\n",
    "# # Start the export task\n",
    "# export_csv.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Wait loop to see if the data has finished exporting by checking with the server-side\n",
    "# prev_task_status = ee.data.getTaskStatus(export_csv.id)[0][\"state\"]\n",
    "# print(prev_task_status)\n",
    "# while export_csv.active():\n",
    "#     task_status = ee.data.getTaskStatus(export_csv.id)[0][\"state\"]\n",
    "#     if(task_status != prev_task_status):\n",
    "#         print(task_status)\n",
    "#     prev_task_status = task_status\n",
    "#     time.sleep(5)\n",
    "# print(ee.data.getTaskStatus(export_csv.id)[0][\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_task = ee.batch.Export.table.toCloudStorage(\n",
    "    collection=joined_samples, \n",
    "    description=siteSelect+'_'+outputName+'_regression_tree', \n",
    "    bucket='eealr',     \n",
    "    fileFormat='CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize and Compare Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into dataframe\n",
    "# data = pd.read_csv('./gdrive/'+siteSelect+'_'+outputName+'_regression_tree.csv')\n",
    "data = pd.read_csv('gs://eealr/'+siteSelect+'_'+outputName+'_regression_tree.csv')\n",
    "\n",
    "# remove rows that have a value of 0 for the responseBand\n",
    "data = data[data[responseBand] != 0]\n",
    "\n",
    "\n",
    "# Get column data (for plots [0,1] and [1,1])\n",
    "rf = data['rf_'+responseBand]/1000    # divide by 1000 to get properly scaled values for the variable\n",
    "cart = data['cart_'+responseBand]/1000\n",
    "actual = data[responseBand]/1000\n",
    "\n",
    "# Obtain point density to display as a scatterplot (KDE)\n",
    "xy_rf = np.vstack([actual, rf])\n",
    "z_rf = scipy.stats.gaussian_kde(xy_rf)(xy_rf)\n",
    "\n",
    "xy_cart = np.vstack([actual, cart])\n",
    "z_cart = scipy.stats.gaussian_kde(xy_cart)(xy_cart)\n",
    "\n",
    "\n",
    "# Sort by responseBand in ascending order (for plots [0,0] and [1,0] below)\n",
    "data_sorted = data.sort_values(responseBand, axis=0).reset_index(drop=True)\n",
    "rf_sorted = data_sorted['rf_'+responseBand]/1000\n",
    "cart_sorted = data_sorted['cart_'+responseBand]/1000\n",
    "actual_sorted = data_sorted[responseBand]/1000\n",
    "index_sorted = data_sorted.index\n",
    "\n",
    "# Obtain point density for sorted values\n",
    "xy_rf_sorted = np.vstack([actual_sorted, rf_sorted])\n",
    "z_rf_sorted = scipy.stats.gaussian_kde(xy_rf_sorted)(xy_rf_sorted)\n",
    "\n",
    "xy_cart_sorted = np.vstack([actual_sorted, cart_sorted])\n",
    "z_cart_sorted = scipy.stats.gaussian_kde(xy_cart_sorted)(xy_cart_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT & COMPARE PREDICTIONS FROM BOTH REGRESSION TREES\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "xy = np.linspace(0, outputMax, 100)\n",
    "\n",
    "# ax[0,0]\n",
    "fig1 = ax[0,0].scatter(index_sorted, rf_sorted, c=z_rf_sorted, label='random forest')\n",
    "ax[0,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "rf_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, rf_sorted, squared=False)\n",
    "rf_r2_sorted = skl.metrics.r2_score(actual_sorted, rf_sorted)\n",
    "\n",
    "# ax[0,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=rf_rmse_sorted, r2=rf_r2_sorted))\n",
    "ax[0,0].set_xlabel('Index (1000 samples)')\n",
    "ax[0,0].set_ylabel('Random Forest Prediction')\n",
    "ax[0,0].legend()\n",
    "\n",
    "\n",
    "# ax[0,1]\n",
    "ax[0,1].plot(xy, xy, c='r')\n",
    "fig2 = ax[0,1].scatter(rf, actual, c=z_rf)\n",
    "rf_rmse = skl.metrics.mean_squared_error(actual, rf, squared=False)\n",
    "rf_r2 = skl.metrics.r2_score(actual, rf)\n",
    "\n",
    "ax[0,1].title.set_text('RMSE: {rmse:.5f}         R2: {r2:.5f}'.format(rmse=rf_rmse, r2=rf_r2))\n",
    "ax[0,1].set_xlabel('SL2P Prediction')\n",
    "ax[0,1].set_ylabel('Random Forest Prediction')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax1 = fig.add_axes([0.92, 0.55, 0.02, 0.3])\n",
    "fig.colorbar(fig2, cax=cbar_ax1)\n",
    "\n",
    "\n",
    "# ax[1,0]\n",
    "fig3 = ax[1,0].scatter(index_sorted, cart_sorted, c=z_cart_sorted, label='cart')\n",
    "ax[1,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "cart_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, cart_sorted, squared=False)\n",
    "cart_r2_sorted = skl.metrics.r2_score(actual_sorted, cart_sorted)\n",
    "\n",
    "# ax[1,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=cart_rmse_sorted, r2=cart_r2_sorted))\n",
    "ax[1,0].set_xlabel('Index (1000 samples)')\n",
    "ax[1,0].set_ylabel('CART Prediction')\n",
    "ax[1,0].legend()\n",
    "\n",
    "\n",
    "# ax[1,1]\n",
    "ax[1,1].plot(xy, xy, c='r')\n",
    "fig4 = ax[1,1].scatter(cart, actual, c=z_cart)\n",
    "cart_rmse = skl.metrics.mean_squared_error(actual, cart, squared=False)\n",
    "cart_r2 = skl.metrics.r2_score(actual, cart)\n",
    "ax[1,1].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=cart_rmse, r2=cart_r2))\n",
    "ax[1,1].set_xlabel('SL2P Prediction')\n",
    "ax[1,1].set_ylabel('CART Prediction')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax3 = fig.add_axes([0.92, 0.15, 0.02, 0.3])\n",
    "fig.colorbar(fig4, cax=cbar_ax3)\n",
    "\n",
    "# save plot as .png\n",
    "# fig.savefig('./plots/trees/'+siteSelect+'_'+outputName+'_rf_cart_comparison.png')\n",
    "# fig.savefig('gs://eealr/'+siteSelect+'_'+outputName+'_rf_cart_comparison.png')\n",
    "fig.savefig(siteSelect+'_'+outputName+'_rf_cart_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Display map\n",
    "# vis_params = {\n",
    "#     'min': 0,\n",
    "#     'max': outputScale*outputMax}\n",
    "\n",
    "# rf_map = ee.Image(ee_func.displayImage(rf_classified, vis_params['min'], vis_params['max'], mapBounds)).clip(mapBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export directly to EE:\n",
    "# export_task = ee_func.export_collection_to_gee(collection=rf_classified,\n",
    "#                                                num_images=1,\n",
    "#                                                image_names=[siteSelect+'_'+outputName+'_random_forest'],\n",
    "#                                                scale=10,\n",
    "#                                                # asset_folder='users/kateharvey/regression_images',                                            \n",
    "#                                                asset_folder=assetfolder,\n",
    "#                                                data_type=exportDataType,\n",
    "#                                                max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b – NNET (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale image responseBand for export (was multiplied by 1000 for previous steps, so rescale before proceeding)\n",
    "inputImage_tf = inputImage.addBands(inputImage.select(responseBand).divide(1000), overwrite=True)\n",
    "# inputImage.sample(numPixels=100).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the export task on the server side from Earth Engine. Remember that the data will be exported to the google drive of the google\n",
    "# account you used when you initiated the Earth Engine API authentication flow, so ensure that, that accounts drive is synced to the \n",
    "# gdrive folder in the same folder as this script\n",
    "trimmedCollection = alr.trim_data(image=inputImage_tf.updateMask(inputImage_tf.select(responseBand).gt(0)),\n",
    "                                  selected_features=select_features,\n",
    "                                  response_band=responseBand,\n",
    "                                  num_samples=50000,\n",
    "                                  num_partitions=10)\n",
    "\n",
    "# exportData = ee.batch.Export.table.toDrive(collection=trimmedCollection,\n",
    "#                                            description=siteSelect+'_'+outputName+'_nnet_10m',\n",
    "#                                            fileFormat=\"CSV\")\n",
    "\n",
    "# # Start the export data task\n",
    "# exportData.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportData_task = ee.batch.Export.table.toCloudStorage(\n",
    "    collection=trimmedCollection, \n",
    "    description=siteSelect+'_'+outputName+'_nnet', \n",
    "    bucket='eealr',     \n",
    "    fileFormat='CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportData_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and make nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into dataframes\n",
    "# trimmed_data = pd.read_csv('./gdrive/'+siteSelect+'_'+outputName+'_nnet_10m.csv')\n",
    "trimmed_data = pd.read_csv('gs://eealr/'+siteSelect+'_'+outputName+'_nnet.csv')\n",
    "X = trimmed_data.drop(labels=[responseBand, 'system:index', '.geo'], axis=1)\n",
    "y = trimmed_data[responseBand]\n",
    "\n",
    "# Preprocess the input features by standardizing them to a mean of 0 and a standard deviation of 1 for the neural network\n",
    "X = pd.DataFrame(skl.preprocessing.scale(X))\n",
    "print (len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Keras to create a sequential model neural network which only has simple dense layers of the specified number of nodes\n",
    "### record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "model = alr.make_nets(X, y)\n",
    "\n",
    "### check the time used\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict our input data to evaluate the performance (for now)\n",
    "predictions = pd.Series(model.predict(X.to_numpy()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to display as a scatterplot\n",
    "xy_tf = np.vstack([y, predictions])\n",
    "z_tf = scipy.stats.gaussian_kde(xy_tf)(xy_tf)\n",
    "\n",
    "idx_tf = z_tf.argsort()\n",
    "x_tf = y[idx_tf]\n",
    "y_tf = predictions[idx_tf]\n",
    "z_tf = z_tf[idx_tf]\n",
    "\n",
    "rmse_tf = skl.metrics.mean_squared_error(x_tf, y_tf, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tf = np.linspace(0, outputMax, 1000)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,10))\n",
    "\n",
    "fig1 = ax.scatter(x_tf, y_tf, c=z_tf)\n",
    "ax.plot(a_tf, a_tf, c='r')\n",
    "ax.set_xlabel('SL2P {}'.format(responseBand))\n",
    "ax.set_ylabel('LARS + NNET predicted {}'.format(outputName))\n",
    "plt.colorbar(mappable=fig1, ax=ax)\n",
    "\n",
    "ax.title.set_text('LASSO LARS {} PREDICTION               RMSE = {rmse:.3f}'.format(outputName, rmse=rmse_tf))\n",
    "\n",
    "# save plot as .png\n",
    "# fig.savefig('./plots/lars/'+siteSelect+'_'+outputName+'_lars.png')\n",
    "fig.savefig(siteSelect+'_'+outputName+'_lars.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = np.vectorize(alr.elu)\n",
    "softplus = np.vectorize(alr.softplus)\n",
    "softsign = np.vectorize(alr.softsign)\n",
    "relu = np.vectorize(alr.relu)\n",
    "tanh = np.vectorize(alr.tanh)\n",
    "sigmoid = np.vectorize(alr.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=X.shape[0]-1\n",
    "inputs = X.iloc[row, :].to_numpy()\n",
    "print(model.predict(inputs.reshape((-1,5)))[0][0])\n",
    "print(alr.apply_nnet(inputs, model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the neural network to a CSV file to be uploaded to the server side on Google Earth Engine\n",
    "export_data = alr.export_nnet(model, X)\n",
    "with open('nnet.csv', 'w', newline='') as csvfile:\n",
    "    nnet_writer = csv.writer(csvfile)\n",
    "    for layerdata in export_data:\n",
    "        nnet_writer.writerow(layerdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2c – NNET (Tensorflow-Google Cloud)\n",
    "\n",
    "### This step is indepent of step 2b, a seperated step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale image responseBand for export (was multiplied by 1000 for previous steps, so rescale before proceeding)\n",
    "inputImage_tensor = inputImage.addBands(inputImage.select(responseBand).divide(1000), overwrite=True)\n",
    "print (inputImage_tensor.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmedCollection2 = alr.trim_data(image=inputImage_tensor.updateMask(inputImage_tensor.select(responseBand).gt(0)),\n",
    "                                  selected_features=select_features,\n",
    "                                  response_band=responseBand,\n",
    "                                  num_samples=50000,\n",
    "                                  num_partitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the sequence of feature names for later bands input correctly\n",
    "feat=trimmedCollection2.limit(3)\n",
    "tempfeat=feat.toList(3).get(0)\n",
    "indiceName=ee.Feature(tempfeat).toDictionary().keys().getInfo()\n",
    "print (indiceName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Training and testing dataset file names in the Cloud Storage bucket.\n",
    "TRAIN_FILE_PREFIX =siteSelect+'_'+outputName+ '_ALR_training'\n",
    "BUCKET = 'eealr'\n",
    "\n",
    "file_extension = '.tfrecord.gz'\n",
    "TRAIN_FILE_PATH = 'gs://' + BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
    "# The labels, LAI float32, are stored in\n",
    "# this property, set on each point.\n",
    "LABEL = 'estimateLAI'\n",
    "\n",
    "# These names are used to specify properties in the export of training/testing\n",
    "# data and to define the mapping between names and data when reading from\n",
    "# the TFRecord file into a tf.data.Dataset.\n",
    "\n",
    "# BANDS=['EVI2', 'GVI','MSR', 'NDGI','RDVI']\n",
    "\n",
    "BANDS=indiceName[:-1]\n",
    "FEATURE_NAMES = list(BANDS)  \n",
    "FEATURE_NAMES.append(LABEL)\n",
    "\n",
    "# List of fixed-length features, all of which are float32.\n",
    "columns = [\n",
    "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
    "]\n",
    "\n",
    "# Dictionary with feature names as keys, fixed-length features as values.\n",
    "FEATURES_DICT = dict(zip(FEATURE_NAMES, columns))\n",
    "\n",
    "# Where to save the trained model.\n",
    "MODEL_DIR = 'gs://' + BUCKET +'/'+ siteSelect+'_'+outputName+'_ALR_model'\n",
    "# Where to save the EEified model.\n",
    "EEIFIED_DIR = 'gs://' + BUCKET +'/'+siteSelect+'_'+outputName+'_ALR_eeified'\n",
    "# Name of the AI Platform model to be hosted.\n",
    "MODEL_NAME = siteSelect+'_'+outputName+'_ALR_model'\n",
    "# Version of the AI Platform model to be hosted.\n",
    "VERSION_NAME = 'alrDNN1'\n",
    "\n",
    "# CLOUD PROJECT!\n",
    "PROJECT = 'ccmeo-ag-000008'\n",
    "# This is a good region for hosting AI models.\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = ee.batch.Export.table.toCloudStorage(\n",
    "  collection = trimmedCollection2,\n",
    "  description = TRAIN_FILE_PREFIX,\n",
    "  bucket = BUCKET,\n",
    "  fileFormat = 'TFRecord'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "\n",
    "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the predictors dictionary and the label, cast to an `float32`.\n",
    "  \"\"\"\n",
    "  parsed_features = tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "  labels = parsed_features.pop(LABEL)\n",
    "  return parsed_features, tf.cast(labels, tf.float32)\n",
    "\n",
    "\n",
    "def to_tuple(inputs, label):\n",
    "  \"\"\" Convert inputs to a tuple.\n",
    "\n",
    "  Note that the inputs must be a tuple of tensors in the right shape.\n",
    "\n",
    "  Args:\n",
    "    dict: a dictionary of tensors keyed by input name.\n",
    "    label: a tensor storing the response variable.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of tensors: (predictors, label).\n",
    "  \"\"\"\n",
    "  # Values in the tensor are ordered by the list of predictors.\n",
    "  predictors = [inputs.get(k) for k in BANDS]\n",
    "  return (tf.expand_dims(tf.transpose(predictors), 1),\n",
    "          tf.expand_dims(tf.expand_dims(label, 1), 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets from the files.\n",
    "train_dataset = tf.data.TFRecordDataset(TRAIN_FILE_PATH, compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the size of the shuffle buffer.  We can get away with this\n",
    "# because it's a small dataset, but watch out with larger datasets.\n",
    "train_size = 0\n",
    "for _ in iter(train_dataset):\n",
    "  train_size+=1\n",
    "\n",
    "# batch_size = 8\n",
    "# batch_size = 8000\n",
    "batch_size = 8\n",
    "\n",
    "# Map the functions over the datasets to parse and convert to tuples.\n",
    "train_dataset = train_dataset.map(parse_tfrecord)\n",
    "train_dataset = train_dataset.map(to_tuple)\n",
    "train_dataset = train_dataset.shuffle(train_size).batch(batch_size)\n",
    "\n",
    "# Print the first parsed record to check.\n",
    "from pprint import pprint\n",
    "pprint(iter(train_dataset).next())\n",
    "\n",
    "print (train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "from tensorflow import keras\n",
    "\n",
    "### record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "model = tf.keras.models.Sequential([ \n",
    "        # tf.keras.layers.Input((1, 1, len(BANDS))),\n",
    "        # tf.keras.layers.Dense(5),\n",
    "        tf.keras.layers.Dense(5, input_shape=(1, 1, len(BANDS))),\n",
    "        # tf.keras.layers.Dense(5, input_shape=([None, None, len(BANDS)])),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dense(4, activation=\"softsign\"),\n",
    "        tf.keras.layers.Dense(3, activation=\"softsign\"),\n",
    "        tf.keras.layers.Dense(2, activation=\"softsign\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Nadam(), loss='mse', metrics=['mse', 'mae'])\n",
    "\n",
    "# Fitting the model to our trimmed data\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit(x=train_dataset, \n",
    "               epochs=100,            \n",
    "               verbose=1)\n",
    "### check the time used\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_DIR, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(MODEL_DIR, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "  input_name = v.name\n",
    "  break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "  output_name = v.name\n",
    "  break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"estimateLAI\"}) + \"'\"\n",
    "print(input_dict)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!earthengine set_project {PROJECT}\n",
    "!earthengine model prepare --source_dir {MODEL_DIR} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --region {REGION}\n",
    "\n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --region {REGION} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --origin {EEIFIED_DIR} \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --runtime-version=2.7 \\\n",
    "  --python-version=3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the hosted model from Earth Engine\n",
    "Now that the model is hosted on AI Platform, point Earth Engine to it and make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj =inputImage.select(responseBand).projection();\n",
    "# print (proj.getInfo())\n",
    "print (proj.crs().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into an array image for input to the model.\n",
    "# EVI2\tGVI\tMSR\tNDGI\tRDVI\n",
    "# array_image = inputImage.select(select_features).float().toArray()\n",
    "# array_image = inputImage.select('EVI2','GVI','MSR','NDGI','RDVI').float().toArray()\n",
    "array_image = inputImage.select(BANDS).unmask(0).float().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the model hosted on AI Platform.  If you specified a region other\n",
    "# than the default (us-central1) at model creation, specify it here.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName=PROJECT,\n",
    "    modelName=MODEL_NAME,\n",
    "    version=VERSION_NAME,\n",
    "    region=REGION,\n",
    "    # Can be anything, but don't make it too big.\n",
    "    inputTileSize=[8,8],\n",
    "    # Keep this the same as your training data, very important step.    \n",
    "    # proj=ee.Projection('EPSG:32620').atScale(10),\n",
    "    proj=ee.Projection(proj.crs().getInfo()).atScale(10),\n",
    "    fixInputProj=True,\n",
    "    # Note the names here need to match what you specified in the\n",
    "    # output dictionary you passed to the EEifier.\n",
    "    outputBands={'tf_'+responseBand: {\n",
    "        'type': ee.PixelType.float(),\n",
    "        'dimensions': 1\n",
    "      }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output prediction\n",
    "# get the input data mask for later to mask the output\n",
    "mask=inputImage.select(responseBand).mask();\n",
    "#mask the output\n",
    "predictions = model.predictImage(array_image).arrayGet([0]).updateMask(mask)\n",
    "# print (predictions.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get map IDs for display in folium.\n",
    "prediction_vis = {'min': 0, 'max': 10}\n",
    "prediction_mapid = predictions.getMapId(prediction_vis)\n",
    "\n",
    "# Visualize the input imagery and the predictions.\n",
    "map = folium.Map(location=[50,-87], zoom_start=11)\n",
    "folium.TileLayer(\n",
    "  tiles=prediction_mapid['tile_fetcher'].url_format,\n",
    "  attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "  overlay=True,\n",
    "  name='LAI',\n",
    ").add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_image = 'projects/ccmeo-ag-000008/assets/google/lai_prediction_image'\n",
    "\n",
    "# image_task = ee.batch.Export.image.toAsset(\n",
    "#   image = predictions, \n",
    "#   description = 'LAI_predicted', \n",
    "#   assetId = 'users/ganghong/LAIpredictNorm',##export_image, \n",
    "#   region = inputImage.select('EVI2').geometry(),\n",
    "#   scale = 10,\n",
    "#   maxPixels = 1e13\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_task = ee_func.export_collection_to_gee(collection=predictions,\n",
    "#                                                num_images=1,\n",
    "#                                                image_names=[siteSelect+'_'+outputName+'_tf'],\n",
    "#                                                scale=10,                                                                                    \n",
    "#                                                asset_folder=assetfolder,\n",
    "#                                                data_type='float',\n",
    "#                                                max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_folder = 'projects/ccmeo-ag-000008/assets/ALR'\n",
    "# export_image = 'projects/google/logistic_demo_image'\n",
    "# Export directly to EE:\n",
    "export_task = ee_func.export_collection_to_gee(collection=predictions,\n",
    "                                               num_images=1,\n",
    "                                               image_names=[siteSelect+'_'+outputName+'_tf'],\n",
    "                                               scale=10,\n",
    "                                               # asset_folder='users/kateharvey/regression_images',                                            \n",
    "                                               asset_folder=cloud_folder,\n",
    "                                               data_type='float',\n",
    "                                               max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from assets\n",
    "# tf_class = ee.Image(assetfolder+'/'+siteSelect+'_'+outputName+'_tf')\n",
    "tf_class = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_tf')\n",
    "cart_class = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_cart')\n",
    "rf_class = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_rf')\n",
    "# composite a image with estimation bands\n",
    "joined_image2 = inputImage.select(responseBand).addBands(tf_class.select('tf_'+responseBand)).addBands(cart_class.select('cart_'+responseBand)).addBands(rf_class.select('rf_'+responseBand))\n",
    "\n",
    "# using same random seed as training_data for cart and rf, get 2000 samples and discard the first 1000, leaving 1000 different samples for cross-validation\n",
    "# this sampling method ensures no overlap between training and testing datasets\n",
    "joined_samples2 = ee.FeatureCollection(joined_image2.sample(numPixels=2000, seed=1).toList(2000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_task2 = ee.batch.Export.table.toCloudStorage(\n",
    "    collection=joined_samples2, \n",
    "    description=siteSelect+'_'+outputName+'_rt', \n",
    "    bucket='eealr',     \n",
    "    fileFormat='CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_task2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into dataframe\n",
    "data = pd.read_csv('gs://eealr/'+siteSelect+'_'+outputName+'_rt.csv')\n",
    "\n",
    "# remove rows that have a value of 0 for the responseBand\n",
    "data = data[data[responseBand] != 0]\n",
    "\n",
    "rf = data['rf_'+responseBand]/1000    # divide by 1000 to get properly scaled values for the variable\n",
    "cart = data['cart_'+responseBand]/1000\n",
    "actual = data[responseBand]/1000\n",
    "tf = data['tf_'+responseBand]\n",
    "# Obtain point density to display as a scatterplot (KDE)\n",
    "xy_rf = np.vstack([actual, rf])\n",
    "z_rf = scipy.stats.gaussian_kde(xy_rf)(xy_rf)\n",
    "\n",
    "xy_cart = np.vstack([actual, cart])\n",
    "z_cart = scipy.stats.gaussian_kde(xy_cart)(xy_cart)\n",
    "\n",
    "xy_tf = np.vstack([actual, tf])\n",
    "z_tf = scipy.stats.gaussian_kde(xy_tf)(xy_tf)\n",
    "\n",
    "# Sort by responseBand in ascending order (for plots [0,0] and [1,0], [2,0] below)\n",
    "data_sorted = data.sort_values(responseBand, axis=0).reset_index(drop=True)\n",
    "rf_sorted = data_sorted['rf_'+responseBand]/1000\n",
    "cart_sorted = data_sorted['cart_'+responseBand]/1000\n",
    "tf_sorted = data_sorted['tf_'+responseBand]\n",
    "actual_sorted = data_sorted[responseBand]/1000\n",
    "index_sorted = data_sorted.index\n",
    "\n",
    "# Obtain point density for sorted values\n",
    "xy_rf_sorted = np.vstack([actual_sorted, rf_sorted])\n",
    "z_rf_sorted = scipy.stats.gaussian_kde(xy_rf_sorted)(xy_rf_sorted)\n",
    "\n",
    "xy_cart_sorted = np.vstack([actual_sorted, cart_sorted])\n",
    "z_cart_sorted = scipy.stats.gaussian_kde(xy_cart_sorted)(xy_cart_sorted)\n",
    "\n",
    "xy_tf_sorted = np.vstack([actual_sorted, tf_sorted])\n",
    "z_tf_sorted = scipy.stats.gaussian_kde(xy_tf_sorted)(xy_tf_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT & COMPARE PREDICTIONS FROM BOTH REGRESSION TREES\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12,18))\n",
    "xy = np.linspace(0, outputMax, 100)\n",
    "\n",
    "# ax[0,0]\n",
    "fig1 = ax[0,0].scatter(index_sorted, rf_sorted, c=z_rf_sorted, label='random forest')\n",
    "ax[0,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "rf_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, rf_sorted, squared=False)\n",
    "rf_r2_sorted = skl.metrics.r2_score(actual_sorted, rf_sorted)\n",
    "\n",
    "# ax[0,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=rf_rmse_sorted, r2=rf_r2_sorted))\n",
    "ax[0,0].set_xlabel('Index (1000 samples)')\n",
    "ax[0,0].set_ylabel('Random Forest Prediction')\n",
    "ax[0,0].legend()\n",
    "\n",
    "# ax[0,1]\n",
    "ax[0,1].plot(xy, xy, c='r')\n",
    "fig2 = ax[0,1].scatter(rf, actual, c=z_rf)\n",
    "rf_rmse = skl.metrics.mean_squared_error(actual, rf, squared=False)\n",
    "rf_r2 = skl.metrics.r2_score(actual, rf)\n",
    "\n",
    "ax[0,1].title.set_text('RMSE: {rmse:.3f}         R2: {r2:.3f}'.format(rmse=rf_rmse, r2=rf_r2))\n",
    "ax[0,1].set_xlabel('SL2P Prediction')\n",
    "ax[0,1].set_ylabel('Random Forest Prediction')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax1 = fig.add_axes([0.92, 0.67, 0.02, 0.2])\n",
    "fig.colorbar(fig2, cax=cbar_ax1)\n",
    "\n",
    "# ax[1,0]\n",
    "fig3 = ax[1,0].scatter(index_sorted, cart_sorted, c=z_cart_sorted, label='cart')\n",
    "ax[1,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "cart_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, cart_sorted, squared=False)\n",
    "cart_r2_sorted = skl.metrics.r2_score(actual_sorted, cart_sorted)\n",
    "\n",
    "# ax[1,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=cart_rmse_sorted, r2=cart_r2_sorted))\n",
    "ax[1,0].set_xlabel('Index (1000 samples)')\n",
    "ax[1,0].set_ylabel('CART Prediction')\n",
    "ax[1,0].legend()\n",
    "\n",
    "# ax[1,1]\n",
    "ax[1,1].plot(xy, xy, c='r')\n",
    "fig4 = ax[1,1].scatter(cart, actual, c=z_cart)\n",
    "cart_rmse = skl.metrics.mean_squared_error(actual, cart, squared=False)\n",
    "cart_r2 = skl.metrics.r2_score(actual, cart)\n",
    "ax[1,1].title.set_text('RMSE: {rmse:.3f}          R2: {r2:.3f}'.format(rmse=cart_rmse, r2=cart_r2))\n",
    "ax[1,1].set_xlabel('SL2P Prediction')\n",
    "ax[1,1].set_ylabel('CART Prediction')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax2 = fig.add_axes([0.92, 0.4, 0.02, 0.2])\n",
    "fig.colorbar(fig4, cax=cbar_ax2)\n",
    "\n",
    "# ax[2,0]\n",
    "fig5 = ax[2,0].scatter(index_sorted, tf_sorted, c=z_tf_sorted, label='tf')\n",
    "ax[2,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "tf_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, tf_sorted, squared=False)\n",
    "tf_r2_sorted = skl.metrics.r2_score(actual_sorted, tf_sorted)\n",
    "\n",
    "# ax[1,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=cart_rmse_sorted, r2=cart_r2_sorted))\n",
    "ax[2,0].set_xlabel('Index (1000 samples)')\n",
    "ax[2,0].set_ylabel('TensorFlow Prediction')\n",
    "ax[2,0].legend()\n",
    "\n",
    "# ax[2,1]\n",
    "ax[2,1].plot(xy, xy, c='r')\n",
    "fig6 = ax[2,1].scatter(tf, actual, c=z_tf)\n",
    "tf_rmse = skl.metrics.mean_squared_error(actual, tf, squared=False)\n",
    "tf_r2 = skl.metrics.r2_score(actual, tf)\n",
    "ax[2,1].title.set_text('RMSE: {rmse:.3f}          R2: {r2:.3f}'.format(rmse=tf_rmse, r2=tf_r2))\n",
    "ax[2,1].set_xlabel('SL2P Prediction')\n",
    "ax[2,1].set_ylabel('TensorFlow Prediction')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax3 = fig.add_axes([0.92, 0.13, 0.02, 0.2])\n",
    "fig.colorbar(fig6, cax=cbar_ax3)\n",
    "\n",
    "# save plot as .png\n",
    "# fig.savefig('./plots/trees/'+siteSelect+'_'+outputName+'_rf_cart_comparison.png')\n",
    "# fig.savefig('gs://eealr/'+siteSelect+'_'+outputName+'_rf_cart_comparison.png')\n",
    "fig.savefig(siteSelect+'_'+outputName+'_rf_cart_tf_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export directly to EE:\n",
    "# export_task = ee_func.export_collection_to_gee(collection=ee.ImageCollection(predictions),\n",
    "#                                                num_images=1,\n",
    "#                                                image_names=[siteSelect+'_'+outputName+'_cloud'],                                             \n",
    "#                                                scale=10,                                               \n",
    "#                                                # asset_folder='users/kateharvey/regression_images',                                            \n",
    "#                                                asset_folder=assetfolder,\n",
    "#                                                data_type=exportDataType,\n",
    "#                                                max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code package setup for submitting trainging on AI platform\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_PATH = siteSelect+'_'+outputName+'_ALR_platformGPU'\n",
    "\n",
    "!ls -l\n",
    "!mkdir {PACKAGE_PATH}\n",
    "!touch {PACKAGE_PATH}/__init__.py\n",
    "!ls -l {PACKAGE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "These variables need to be stored in a place where other code can access them.  Here we'll use the `%%writefile` command to write the contents of the code cell to a file called `config.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PACKAGE_PATH}/config.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### need change the following three items for submitting training on Cloud\n",
    "siteSelect='FoxCreek'\n",
    "outputName='LAI'\n",
    "BANDS=['EVI2', 'GNDVI', 'GVI', 'MSR', 'RDVI'] ## based on the indiceName excluding last item\n",
    "\n",
    "#  PROJECT HERE!\n",
    "PROJECT = 'ccmeo-ag-000008'\n",
    "# BUCKET HERE!\n",
    "BUCKET = 'eealr'\n",
    "#  a region for hosting AI models.\n",
    "REGION = 'northamerica-northeast1'\n",
    "# Specify names of output locations in Cloud Storage.\n",
    "\n",
    "FOLDER = siteSelect+'_'+outputName+'_ALR_AIGPU'\n",
    "JOB_DIR = 'gs://' + BUCKET + '/' + FOLDER + '/trainer'\n",
    "MODEL_DIR = JOB_DIR + '/model'\n",
    "LOGS_DIR = JOB_DIR + '/logs'\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEIFIED_DIR = JOB_DIR + '/eeified'\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "TRAIN_FILE_PREFIX = siteSelect+'_'+outputName+'_ALR_training'\n",
    "\n",
    "file_extension = '.tfrecord.gz'\n",
    "TRAIN_FILE_PATH = 'gs://' + BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
    "\n",
    "# The labels, biophysical parameter, float32, are stored in\n",
    "# this property, set on each point.\n",
    "LABEL = 'estimateLAI'\n",
    "# These names are used to specify properties in the export of training/testing\n",
    "# data and to define the mapping between names and data when reading from\n",
    "# the TFRecord file into a tf.data.Dataset.\n",
    "\n",
    "# BANDS=['EVI2','GVI','MSR','NDGI','RDVI']\n",
    "# BANDS=['B4', 'GVI', 'MSR', 'RDVI', 'SGI']\n",
    "FEATURE_NAMES = list(BANDS)  \n",
    "FEATURE_NAMES.append(LABEL)\n",
    "\n",
    "# List of fixed-length features, all of which are float32.\n",
    "columns = [\n",
    "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
    "]\n",
    "\n",
    "# Dictionary with feature names as keys, fixed-length features as values.\n",
    "FEATURES_DICT = dict(zip(FEATURE_NAMES, columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the written file has the expected contents and is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {PACKAGE_PATH}/config.py\n",
    "\n",
    "# from ALR_platform import config, neeed change platform manually\n",
    "from FoxCreek_LAI_ALR_platformGPU import config\n",
    "\n",
    "print('\\n\\n', config.EPOCHS)\n",
    "print (config.MODEL_DIR)\n",
    "print (config.EEIFIED_DIR)\n",
    "print (config.FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data, evaluation data and model\n",
    "\n",
    "The following is code to load training/label data and the model.  Write this into `model.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PACKAGE_PATH}/model.py\n",
    "\n",
    "from . import config\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import metrics\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import optimizers\n",
    "# import pandas as pd\n",
    "\n",
    "# Dataset loading functions\n",
    "def get_training_dataset():\n",
    "    # # Load datasets from the files.\n",
    "    dataset = tf.data.TFRecordDataset(config.TRAIN_FILE_PATH, compression_type='GZIP')\n",
    "    return dataset\n",
    "\n",
    "def get_model(BANDS):  \n",
    "    LAI_model = tf.keras.models.Sequential([\n",
    "            # tf.keras.layers.Input((1, 1, len(BANDS))),\n",
    "            # tf.keras.layers.Dense(5),\n",
    "            tf.keras.layers.Dense(5, input_shape=(1, 1, len(BANDS))),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dense(4, activation=\"softsign\"),\n",
    "            tf.keras.layers.Dense(3, activation=\"softsign\"),\n",
    "            tf.keras.layers.Dense(2, activation=\"softsign\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        # Compiling the model to minimize the mean squared error loss function and use the NADAM optimizer\n",
    "    LAI_model.compile(optimizer=tf.keras.optimizers.Nadam(), loss='mse', metrics=['mse', 'mae'])\n",
    "    return LAI_model\n",
    "\n",
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "\n",
    "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the predictors dictionary and the label, cast to an `float32`.\n",
    "  \"\"\"\n",
    "  parsed_features = tf.io.parse_single_example(example_proto, config.FEATURES_DICT)\n",
    "  labels = parsed_features.pop(config.LABEL)\n",
    "  return parsed_features, tf.cast(labels, tf.float32)\n",
    "\n",
    "\n",
    "def to_tuple(inputs, label):\n",
    "  \"\"\" Convert inputs to a tuple.\n",
    "\n",
    "  Note that the inputs must be a tuple of tensors in the right shape.\n",
    "\n",
    "  Args:\n",
    "    dict: a dictionary of tensors keyed by input name.\n",
    "    label: a tensor storing the response variable.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of tensors: (predictors, label).\n",
    "  \"\"\"\n",
    "  # Values in the tensor are ordered by the list of predictors.\n",
    " \n",
    "  predictors = [inputs.get(k) for k in config.BANDS]\n",
    "  return (tf.expand_dims(tf.transpose(predictors), 1),\n",
    "          tf.expand_dims(tf.expand_dims(label, 1), 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training task\n",
    "\n",
    "The following will create `task.py`, which will get the training and eval data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PACKAGE_PATH}/task.py\n",
    "\n",
    "from . import config\n",
    "from . import model\n",
    "import tensorflow as tf\n",
    "# import pandas\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    train_dataset=model.get_training_dataset()\n",
    "    train_size = 0\n",
    "    for _ in iter(train_dataset):\n",
    "      train_size+=1\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    # Map the functions over the datasets to parse and convert to tuples.\n",
    "    train_dataset = train_dataset.map(model.parse_tfrecord)\n",
    "    train_dataset = train_dataset.map(model.to_tuple)\n",
    "    train_dataset = train_dataset.shuffle(train_size).batch(batch_size)\n",
    "\n",
    "    m = model.get_model(config.BANDS)    \n",
    "    m.fit(train_dataset, epochs=config.EPOCHS, verbose=1)  \n",
    "    m.save(config.MODEL_DIR, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit the package to AI Platform for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# JOB_NAME = 'Geraldton'+'_'+'LAI'+'_ALR_training_job_' + str(int(time.time()))\n",
    "# TRAINER_PACKAGE_PATH = 'Geraldton'+'_'+'LAI'+'_ALR_platform'\n",
    "# MAIN_TRAINER_MODULE = 'Geraldton'+'_'+'LAI'+'_ALR_platform.task'\n",
    "# siteSelect='Kitchener'\n",
    "# outputName='LAI'\n",
    "JOB_NAME = siteSelect+'_'+outputName+'_ALR_training_job_GPU_' + str(int(time.time()))\n",
    "TRAINER_PACKAGE_PATH = siteSelect+'_'+outputName+'_ALR_platformGPU'\n",
    "MAIN_TRAINER_MODULE = siteSelect+'_'+outputName+'_ALR_platformGPU.task'\n",
    "REGION = 'northamerica-northeast1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "    --job-dir {config.JOB_DIR}  \\\n",
    "    --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "    --module-name {MAIN_TRAINER_MODULE} \\\n",
    "    --region {REGION} \\\n",
    "    --project {config.PROJECT} \\\n",
    "    --runtime-version 2.7\\\n",
    "    --python-version 3.7 \\\n",
    "    --scale-tier custom \\\n",
    "    --master-machine-type n1-highcpu-16\\\n",
    "    --master-accelerator count=1,type=NVIDIA-TESLA-P4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the training job\n",
    "\n",
    "There's not much more to do until the model is finished training (~24 hours), but it's fun and useful to monitor its progress. You can do that programmatically with another `gcloud` command.  The output of that command can be read into an `IPython.utils.text.SList` from which the `state` is extracted and ensured to be `SUCCEEDED`.  Or you can monitor it from the [AI Platform jobs page](http://console.cloud.google.com/ai-platform/jobs) on the Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ccmeo-ag-000008'\n",
    "desc = !gcloud ai-platform jobs describe {JOB_NAME} --project {PROJECT}\n",
    "print (desc)\n",
    "state = desc.grep('state:')[0].split(':')[1].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accommodate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64). \n",
    "\n",
    "## `earthengine model prepare`\n",
    "The EEification process is using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(config.MODEL_DIR, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "  input_name = v.name\n",
    "  break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "  output_name = v.name\n",
    "  break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"estimateLAI\"}) + \"'\"\n",
    "print(input_dict)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (config.MODEL_DIR)\n",
    "# print (config.EEIFIED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to set the project before using the model prepare command.\n",
    "!earthengine set_project {PROJECT}\n",
    "!earthengine model prepare --source_dir {config.MODEL_DIR} --dest_dir {config.EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference using the trained model in Earth Engine\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile config.yaml\n",
    "# autoScaling:\n",
    "#     minNodes: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = siteSelect+'_'+outputName+'_ALR_platform'\n",
    "VERSION_NAME = 'v' + str(int(time.time()))\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \\\n",
    "  --project {PROJECT} \\\n",
    "  --region {REGION}\n",
    "\n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --project {config.PROJECT} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --region {REGION} \\\n",
    "  --origin {config.EEIFIED_DIR} \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --runtime-version 2.7 \\\n",
    "  --python-version 3.7 \\\n",
    "  # --config=config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  now connect Earth Engine directly to the trained model for inference.  \n",
    "\n",
    "## `ee.Model.fromAiPlatformPredictor`\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, need to know the name and version.\n",
    "\n",
    "### Inputs\n",
    "to create an array-valued input from the scaled data and use that for input.  \n",
    "\n",
    "### Outputs\n",
    "The output is a single float band ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into an array image for input to the model.\n",
    "array_image2 = inputImage.select(BANDS).unmask(0).float().toArray()\n",
    "# get the input data mask for later to mask the output\n",
    "mask=inputImage.select('estimateLAI').mask();\n",
    "# get the projection of the input image required in MODEL predictor\n",
    "proj2 =inputImage.select(responseBand).projection();\n",
    "print (proj2.crs().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the model hosted on AI Platform. Output projection neeed be modified in the code below.\n",
    "# MODEL_NAME = 'ALR_platform'\n",
    "# VERSION_NAME='v1644344011'\n",
    "REGION = 'northamerica-northeast1'\n",
    "PROJECT = 'ccmeo-ag-000008'\n",
    "model_alr = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName=PROJECT,\n",
    "    modelName=MODEL_NAME,\n",
    "    version=VERSION_NAME,\n",
    "    region=REGION,\n",
    "    # Can be anything, but don't make it too big.\n",
    "    inputTileSize=[8,8],\n",
    "    # Keep this the same as your training data.\n",
    "    # proj=ee.Projection('EPSG:4326').atScale(10),\n",
    "    proj=ee.Projection(proj2.crs().getInfo()).atScale(10),\n",
    "    fixInputProj=True,\n",
    "    # Note the names here need to match what you specified in the\n",
    "    # output dictionary you passed to the EEifier.\n",
    "    outputBands={'tf_'+responseBand: {\n",
    "        'type': ee.PixelType.float(),\n",
    "        'dimensions': 1\n",
    "      }\n",
    "    },\n",
    ")\n",
    "# print (model_alr.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output prediction result.\n",
    "predictions2 = model_alr.predictImage(array_image2).arrayGet([0]).updateMask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get map IDs for display in folium.\n",
    "predictions_vis2 = {'min': 0, 'max': 10}\n",
    "predictions_mapid2 = predictions2.getMapId(predictions_vis2)\n",
    "\n",
    "# Visualize the input imagery and the predictions.\n",
    "map = folium.Map(location=[50, -87], zoom_start=11)\n",
    "folium.TileLayer(\n",
    "  tiles=predictions_mapid2['tile_fetcher'].url_format,\n",
    "  attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "  overlay=True,\n",
    "  name='LAI',\n",
    ").add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_task = ee_func.export_collection_to_gee(collection=predictions2,\n",
    "#                                                num_images=1,\n",
    "#                                                image_names=[siteSelect+'_'+outputName+'_AI_platform_tf'],\n",
    "#                                                scale=10,                                                                                    \n",
    "#                                                asset_folder=assetfolder,\n",
    "#                                                data_type='float',\n",
    "#                                                max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_folder = 'projects/ccmeo-ag-000008/assets/ALR'\n",
    "# export_image = 'projects/google/logistic_demo_image'\n",
    "# Export directly to EE:\n",
    "export_task = ee_func.export_collection_to_gee(collection=predictions2,\n",
    "                                               num_images=1,\n",
    "                                               image_names=[siteSelect+'_'+outputName+'_ALR_platform_tf'],\n",
    "                                               scale=10,\n",
    "                                               # asset_folder='users/kateharvey/regression_images',                                            \n",
    "                                               asset_folder=cloud_folder,\n",
    "                                               data_type='float',\n",
    "                                               max_pixels=1e13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing batch plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from assets\n",
    "# tf_class = ee.Image(assetfolder+'/'+siteSelect+'_'+outputName+'_tf')\n",
    "tf_class = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_tf_1000')\n",
    "cart_class = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_cart')\n",
    "rf_class = ee.Image(cloud_folder+'/'+siteSelect+'_'+outputName+'_rf')\n",
    "# composite a image with estimation bands\n",
    "joined_image2 = inputImage.select(responseBand).addBands(tf_class.select('tf_'+responseBand)).addBands(cart_class.select('cart_'+responseBand)).addBands(rf_class.select('rf_'+responseBand))\n",
    "\n",
    "# using same random seed as training_data for cart and rf, get 2000 samples and discard the first 1000, leaving 1000 different samples for cross-validation\n",
    "# this sampling method ensures no overlap between training and testing datasets\n",
    "joined_samples2 = ee.FeatureCollection(joined_image2.sample(numPixels=2000, seed=1).toList(2000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_task2 = ee.batch.Export.table.toCloudStorage(\n",
    "    collection=joined_samples2, \n",
    "    description=siteSelect+'_'+outputName+'_rttest2', \n",
    "    bucket='eealr',     \n",
    "    fileFormat='CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_task2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into dataframe\n",
    "data = pd.read_csv('gs://eealr/'+siteSelect+'_'+outputName+'_rttest2.csv')\n",
    "\n",
    "# remove rows that have a value of 0 for the responseBand\n",
    "data = data[data[responseBand] != 0]\n",
    "\n",
    "rf = data['rf_'+responseBand]/1000    # divide by 1000 to get properly scaled values for the variable\n",
    "cart = data['cart_'+responseBand]/1000\n",
    "actual = data[responseBand]/1000\n",
    "tf = data['tf_'+responseBand]\n",
    "# Obtain point density to display as a scatterplot (KDE)\n",
    "xy_rf = np.vstack([actual, rf])\n",
    "z_rf = scipy.stats.gaussian_kde(xy_rf)(xy_rf)\n",
    "\n",
    "xy_cart = np.vstack([actual, cart])\n",
    "z_cart = scipy.stats.gaussian_kde(xy_cart)(xy_cart)\n",
    "\n",
    "xy_tf = np.vstack([actual, tf])\n",
    "z_tf = scipy.stats.gaussian_kde(xy_tf)(xy_tf)\n",
    "\n",
    "# Sort by responseBand in ascending order (for plots [0,0] and [1,0], [2,0] below)\n",
    "data_sorted = data.sort_values(responseBand, axis=0).reset_index(drop=True)\n",
    "rf_sorted = data_sorted['rf_'+responseBand]/1000\n",
    "cart_sorted = data_sorted['cart_'+responseBand]/1000\n",
    "tf_sorted = data_sorted['tf_'+responseBand]\n",
    "actual_sorted = data_sorted[responseBand]/1000\n",
    "index_sorted = data_sorted.index\n",
    "\n",
    "# Obtain point density for sorted values\n",
    "xy_rf_sorted = np.vstack([actual_sorted, rf_sorted])\n",
    "z_rf_sorted = scipy.stats.gaussian_kde(xy_rf_sorted)(xy_rf_sorted)\n",
    "\n",
    "xy_cart_sorted = np.vstack([actual_sorted, cart_sorted])\n",
    "z_cart_sorted = scipy.stats.gaussian_kde(xy_cart_sorted)(xy_cart_sorted)\n",
    "\n",
    "xy_tf_sorted = np.vstack([actual_sorted, tf_sorted])\n",
    "z_tf_sorted = scipy.stats.gaussian_kde(xy_tf_sorted)(xy_tf_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT & COMPARE PREDICTIONS FROM BOTH REGRESSION TREES\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12,18))\n",
    "xy = np.linspace(0, outputMax, 100)\n",
    "\n",
    "# ax[0,0]\n",
    "fig1 = ax[0,0].scatter(index_sorted, rf_sorted, c=z_rf_sorted, label='random forest')\n",
    "ax[0,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "rf_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, rf_sorted, squared=False)\n",
    "rf_r2_sorted = skl.metrics.r2_score(actual_sorted, rf_sorted)\n",
    "\n",
    "# ax[0,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=rf_rmse_sorted, r2=rf_r2_sorted))\n",
    "ax[0,0].set_xlabel('Index (1000 samples)')\n",
    "ax[0,0].set_ylabel('Random Forest Prediction')\n",
    "ax[0,0].legend()\n",
    "\n",
    "# ax[0,1]\n",
    "ax[0,1].plot(xy, xy, c='r')\n",
    "fig2 = ax[0,1].scatter(rf, actual, c=z_rf)\n",
    "rf_rmse = skl.metrics.mean_squared_error(actual, rf, squared=False)\n",
    "rf_r2 = skl.metrics.r2_score(actual, rf)\n",
    "\n",
    "ax[0,1].title.set_text('RMSE: {rmse:.3f}         R2: {r2:.3f}'.format(rmse=rf_rmse, r2=rf_r2))\n",
    "ax[0,1].set_xlabel('SL2P Prediction')\n",
    "ax[0,1].set_ylabel('Random Forest Prediction')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax1 = fig.add_axes([0.92, 0.67, 0.02, 0.2])\n",
    "fig.colorbar(fig2, cax=cbar_ax1)\n",
    "\n",
    "# ax[1,0]\n",
    "fig3 = ax[1,0].scatter(index_sorted, cart_sorted, c=z_cart_sorted, label='cart')\n",
    "ax[1,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "cart_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, cart_sorted, squared=False)\n",
    "cart_r2_sorted = skl.metrics.r2_score(actual_sorted, cart_sorted)\n",
    "\n",
    "# ax[1,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=cart_rmse_sorted, r2=cart_r2_sorted))\n",
    "ax[1,0].set_xlabel('Index (1000 samples)')\n",
    "ax[1,0].set_ylabel('CART Prediction')\n",
    "ax[1,0].legend()\n",
    "\n",
    "# ax[1,1]\n",
    "ax[1,1].plot(xy, xy, c='r')\n",
    "fig4 = ax[1,1].scatter(cart, actual, c=z_cart)\n",
    "cart_rmse = skl.metrics.mean_squared_error(actual, cart, squared=False)\n",
    "cart_r2 = skl.metrics.r2_score(actual, cart)\n",
    "ax[1,1].title.set_text('RMSE: {rmse:.3f}          R2: {r2:.3f}'.format(rmse=cart_rmse, r2=cart_r2))\n",
    "ax[1,1].set_xlabel('SL2P Prediction')\n",
    "ax[1,1].set_ylabel('CART Prediction')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax2 = fig.add_axes([0.92, 0.4, 0.02, 0.2])\n",
    "fig.colorbar(fig4, cax=cbar_ax2)\n",
    "\n",
    "# ax[2,0]\n",
    "fig5 = ax[2,0].scatter(index_sorted, tf_sorted, c=z_tf_sorted, label='tf')\n",
    "ax[2,0].scatter(index_sorted, actual_sorted, c='r', s=1, label='sl2p estimate')\n",
    "tf_rmse_sorted = skl.metrics.mean_squared_error(actual_sorted, tf_sorted, squared=False)\n",
    "tf_r2_sorted = skl.metrics.r2_score(actual_sorted, tf_sorted)\n",
    "\n",
    "# ax[1,0].title.set_text('RMSE: {rmse:.5f}          R2: {r2:.5f}'.format(rmse=cart_rmse_sorted, r2=cart_r2_sorted))\n",
    "ax[2,0].set_xlabel('Index (1000 samples)')\n",
    "ax[2,0].set_ylabel('TensorFlow Prediction')\n",
    "ax[2,0].legend()\n",
    "\n",
    "# ax[2,1]\n",
    "ax[2,1].plot(xy, xy, c='r')\n",
    "fig6 = ax[2,1].scatter(tf, actual, c=z_tf)\n",
    "tf_rmse = skl.metrics.mean_squared_error(actual, tf, squared=False)\n",
    "tf_r2 = skl.metrics.r2_score(actual, tf)\n",
    "ax[2,1].title.set_text('RMSE: {rmse:.3f}          R2: {r2:.3f}'.format(rmse=tf_rmse, r2=tf_r2))\n",
    "ax[2,1].set_xlabel('SL2P Prediction')\n",
    "ax[2,1].set_ylabel('TensorFlow Prediction')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax3 = fig.add_axes([0.92, 0.13, 0.02, 0.2])\n",
    "fig.colorbar(fig6, cax=cbar_ax3)\n",
    "\n",
    "# save plot as .png\n",
    "# fig.savefig('./plots/trees/'+siteSelect+'_'+outputName+'_rf_cart_comparison.png')\n",
    "# fig.savefig('gs://eealr/'+siteSelect+'_'+outputName+'_rf_cart_comparison.png')\n",
    "fig.savefig(siteSelect+'_'+outputName+'_rf_cart_tf_comparison_8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
